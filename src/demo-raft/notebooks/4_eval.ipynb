{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation of the student and baseline models with the RAFT generated eval dataset split\n",
    "\n",
    "In this notebook, we will use the evaluation dataset synthetically generated in the [](./1_gen.ipynb) notebook using the RAFT method to evaluate both the baseline model and the student model, then compare the two to analyse the impact of the fine-tuning.\n",
    "\n",
    "We introduce the `promptflow-evals` package and built-in evaluators. Then, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n",
    "\n",
    "Finally, we'll draw a diagram showing the performance of the student model against the baseline.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Testing\n",
    "  - Run the baseline model on the evaluation split to get its predictions.\n",
    "  - Run the student model on the evaluation split to get its predictions.\n",
    "- Answers formatting\n",
    "  - Convert the baseline model answers to a format suitable for testing\n",
    "  - Convert the student model answers to a format suitable for testing\n",
    "- Evaluation\n",
    "  - Calculate the metrics (such as accuracy, precision, recall, etc.) based on the predictions from the baseline model.\n",
    "  - Calculate the metrics based on the predictions from the student model.  \n",
    "- Compare metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5781ca1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "![](./doc/raft-process-eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fe581",
   "metadata": {},
   "source": [
    "## Installing requirements\n",
    "\n",
    "The requirements should have been automatically installed if you opened the project in Dev Container or Codespaces, but if not, uncomment the following cell to install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e99e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai azure-ai-evaluation azure-identity promptflow-azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf5d65",
   "metadata": {},
   "source": [
    "## Running time and cost\n",
    "\n",
    "The RAFT evaluation script usually takes a few minutes on the default sample document but can take days on bigger domains depending on the number and size of documents and the number of questions being generated for each chunk.\n",
    "\n",
    "The cost of running this RAFT script on the sample document should be a few dollars. But beware, running it on bigger domains can cost hundreds of dollars if not more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1714204",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5ab54",
   "metadata": {},
   "source": [
    "### Overview\n",
    "![](./doc/raft-process-eval-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f6ee9",
   "metadata": {},
   "source": [
    "\n",
    "### Define variables we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31004ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# User provided values\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Variables passed by previous notebooks\n",
    "load_dotenv(\".env.state\")\n",
    "\n",
    "# Let's capture the initial working directory because the evaluate function will change it\n",
    "dir = os.getcwd()\n",
    "\n",
    "experiment_name = os.getenv(\"DATASET_NAME\")\n",
    "experiment_dir = f\"{dir}/dataset/{experiment_name}-files\"\n",
    "\n",
    "# Dataset generated by the gen notebook that we will evaluate the baseline and student models on\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "\n",
    "# Evaluated answer files\n",
    "dataset_path_hf_eval_answer = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.jsonl\"\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Formatted answer evaluation files\n",
    "dataset_path_eval_answer_student = f\"{experiment_dir}/{experiment_name}-eval.answer.student.jsonl\"\n",
    "dataset_path_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Scored answer files\n",
    "dataset_path_eval_answer_score_student = f\"{experiment_dir}/{experiment_name}-eval.answer.score.student.jsonl\"\n",
    "dataset_path_eval_answer_score_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.baseline.jsonl\"\n",
    "\n",
    "# Scored answer metrics files\n",
    "dataset_path_eval_answer_score_metrics_student = f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.student.json\"\n",
    "dataset_path_eval_answer_score_metrics_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.baseline.json\"\n",
    "\n",
    "BASELINE_OPENAI_DEPLOYMENT = os.getenv(\"BASELINE_OPENAI_DEPLOYMENT\")\n",
    "BASELINE_MODEL_API = os.getenv(\"BASELINE_MODEL_API\")\n",
    "\n",
    "STUDENT_DEPLOYMENT_NAME = os.getenv(\"STUDENT_DEPLOYMENT_NAME\")\n",
    "STUDENT_MODEL_API = os.getenv(\"STUDENT_MODEL_API\")\n",
    "\n",
    "print(f\"Evaluating the student {STUDENT_MODEL_API} model {STUDENT_DEPLOYMENT_NAME} against the baseline {BASELINE_MODEL_API} model {BASELINE_OPENAI_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451ac45",
   "metadata": {},
   "source": [
    "### Run the baseline model on the evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e5f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -f $dataset_path_hf_eval_answer_baseline ] && env $(cat .env .env.state) python .gorilla/raft/eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer_baseline \\\n",
    "    --model $BASELINE_OPENAI_DEPLOYMENT \\\n",
    "    --env-prefix BASELINE \\\n",
    "    --mode $BASELINE_MODEL_API \\\n",
    "    || echo \"Baseline answers file already exists, skipping.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c61dd",
   "metadata": {},
   "source": [
    "### Format baseline answers\n",
    "\n",
    "Convert the baseline model answers to a format suitable for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdda3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_eval_answer_baseline \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_eval_answer_baseline \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed06cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pretty_print_row\n",
    "import pandas as pd\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_baseline, lines=True), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5fe62",
   "metadata": {},
   "source": [
    "### Run the student model on the evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85194f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -f $dataset_path_hf_eval_answer ] && env $(cat .env .env.state) python .gorilla/raft/eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer \\\n",
    "    --model $STUDENT_DEPLOYMENT_NAME \\\n",
    "    --env-prefix STUDENT \\\n",
    "    --mode $STUDENT_MODEL_API \\\n",
    "    || echo \"Student answers file already exists, skipping.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b0827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_json(dataset_path_hf_eval_answer, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b4d8b",
   "metadata": {},
   "source": [
    "### Format student model answers\n",
    "\n",
    "Convert the student model answers to a format suitable for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4a21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_eval_answer \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_eval_answer_student \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c556e",
   "metadata": {},
   "source": [
    "### Student model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff092e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pretty_print_row\n",
    "import pandas as pd\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_student, lines=True), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9fa3f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0958b",
   "metadata": {},
   "source": [
    "### Overview\n",
    "![](./doc/raft-process-eval-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "### Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "| Category       | Evaluator Class           | Notes                                             |\n",
    "|----------------|---------------------------|---------------------------------------------------|\n",
    "| Quality        | GroundednessEvaluator     | Measures how well the answer is entailed by the context and is not hallucinated |\n",
    "|                | RelevanceEvaluator        | How well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. |\n",
    "|                | CoherenceEvaluator        | How well all the sentences fit together and sound naturally as a whole. |\n",
    "|                | FluencyEvaluator          | Quality of individual sentences in the answer, and whether they are well-written and grammatically correct. |\n",
    "|                | SimilarityEvaluator       | Measures the similarity between the predicted answer and the correct answer |\n",
    "| Content Safety | ViolenceEvaluator         |                                                   |\n",
    "|                | SexualEvaluator           |                                                   |\n",
    "|                | SelfHarmEvaluator         |                                                   |\n",
    "|                | HateUnfairnessEvaluator   |                                                   |\n",
    "| Composite      | QAEvaluator               | Built on top of individual quality evaluators.    |\n",
    "|                | ChatEvaluator             | Similar to QAEvaluator but designed for evaluating chat messages. |\n",
    "|                | ContentSafetyEvaluator    | Built on top of individual content safety evaluators. |\n",
    "| Math           | BleuScoreEvaluator        | BLEU Score |\n",
    "|                | RougeScoreEvaluator       | ROUGE Score |\n",
    "|                | F1ScoreEvaluator          | F1 score |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "#### Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3fa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import OpenAIModelConfiguration, AzureOpenAIModelConfiguration\n",
    "\n",
    "openai_base_url = os.environ.get(\"JUDGE_OPENAI_BASE_URL\")\n",
    "azure_endpoint = os.environ.get(\"JUDGE_AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "if openai_base_url:\n",
    "    openai_api_key = os.environ.get(\"JUDGE_OPENAI_API_KEY\")\n",
    "    model = os.environ.get(f\"JUDGE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "    print(f\"openai_base_url={openai_base_url}\")\n",
    "    print(f\"model={model}\")\n",
    "\n",
    "    # Initialize OpenAI Connection\n",
    "    model_config = OpenAIModelConfiguration(\n",
    "        base_url=openai_base_url,\n",
    "        api_key=openai_api_key,\n",
    "        model=model\n",
    "    )\n",
    "    model_config.api_version = None\n",
    "\n",
    "elif azure_endpoint:\n",
    "    azure_deployment = os.environ.get(\"JUDGE_AZURE_OPENAI_DEPLOYMENT\")\n",
    "    api_key = os.environ.get(\"JUDGE_AZURE_OPENAI_API_KEY\")\n",
    "    api_version = os.environ.get(\"JUDGE_OPENAI_API_VERSION\")\n",
    "\n",
    "    print(f\"azure_endpoint={azure_endpoint}\")\n",
    "    print(f\"azure_deployment={azure_deployment}\")\n",
    "    print(f\"api_version={api_version}\")\n",
    "\n",
    "    args = {\n",
    "        'azure_endpoint': azure_endpoint,\n",
    "        'azure_deployment': azure_deployment,\n",
    "        'api_version': api_version,\n",
    "    }\n",
    "    if api_key:\n",
    "        args['api_key'] = api_key\n",
    "\n",
    "    # Initialize Azure OpenAI Connection\n",
    "    model_config = AzureOpenAIModelConfiguration(args)\n",
    "\n",
    "else:\n",
    "    print(\"Couldn't find a judge endpoint environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator, F1ScoreEvaluator, FluencyEvaluator, GroundednessEvaluator, RelevanceEvaluator, SimilarityEvaluator, BleuScoreEvaluator, RougeScoreEvaluator, RougeType\n",
    "\n",
    "explanations = {\n",
    "    \"groundedness\": \"Measures how well the answer is entailed by the context and is not hallucinated\",\n",
    "    \"relevance\": \"How well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\",\n",
    "    \"coherence\": \"How well all the sentences fit together and sound naturally as a whole.\",\n",
    "    \"fluency\": \"Quality of individual sentences in the answer, and whether they are well-written and grammatically correct.\",\n",
    "    \"similarity\": \"Measures the similarity between the predicted answer and the correct answer\",\n",
    "    \"f1_score\": \"Measures the overlap between the predicted answer and the correct answer\",\n",
    "}\n",
    "\n",
    "# Initializing evaluators\n",
    "evaluators = {\n",
    "\n",
    "    # GPT based metrics\n",
    "    \"coherence\" : CoherenceEvaluator(model_config),\n",
    "    \"f1_score\" : F1ScoreEvaluator(),\n",
    "    \"fluency\" : FluencyEvaluator(model_config),\n",
    "    \"groundedness\" : GroundednessEvaluator(model_config),\n",
    "    \"relevance\" : RelevanceEvaluator(model_config),\n",
    "    \"similarity\" : SimilarityEvaluator(model_config),\n",
    "\n",
    "    # Math metrics\n",
    "    \"bleu\" : BleuScoreEvaluator(),\n",
    "    \"rouge_1\" : RougeScoreEvaluator(RougeType.ROUGE_1),\n",
    "    \"rouge_2\" : RougeScoreEvaluator(RougeType.ROUGE_2),\n",
    "\n",
    "#    \"qa\" : QAEvaluator(model_config),\n",
    "#    \"chat\" : ChatEvaluator(model_config),\n",
    "\n",
    "#    \"violence\" : ViolenceEvaluator(model_config),\n",
    "#    \"sexual\" : SexualEvaluator(model_config),\n",
    "#    \"self_harm\" : SelfHarmEvaluator(model_config),\n",
    "#    \"hate_unfairness\" : HateUnfairnessEvaluator(model_config),\n",
    "\n",
    "#    \"content_safety\" : ContentSafetyEvaluator(model_config),\n",
    "#    \"content_safety_chat\" : ContentSafetyChatEvaluator(model_config),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99915b",
   "metadata": {},
   "source": [
    "### Run metrics on a student model answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6247d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_eval_answer_student, lines=True)\n",
    "pretty_print_row(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d90565",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.iloc[1]\n",
    "\n",
    "# Running similarity Evaluator on single input row\n",
    "similarity_score = evaluators[\"similarity\"](\n",
    "    query=sample[\"question\"],\n",
    "    response=sample[\"final_answer\"],\n",
    "    context=sample[\"context\"],\n",
    "    ground_truth=sample[\"gold_final_answer\"],\n",
    ")\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "### Using the Evaluate API to calculate the metrics in bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049643c",
   "metadata": {},
   "source": [
    "### Running the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "def score_dataset(dataset, rows_output_path=None, metrics_output_path=None):\n",
    "    result = evaluate(\n",
    "        data=dataset,\n",
    "        evaluators=evaluators,\n",
    "        # column mapping\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.question}\",\n",
    "                    \"response\": \"${data.final_answer}\",\n",
    "                    \"ground_truth\": \"${data.gold_final_answer}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if rows_output_path:\n",
    "        pd.DataFrame.from_dict(result[\"rows\"]).to_json(rows_output_path, orient=\"records\", lines=True)\n",
    "\n",
    "    if metrics_output_path:\n",
    "        import json\n",
    "        with open(metrics_output_path, \"w\") as f:\n",
    "            json.dump(result['metrics'], f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51dd4c",
   "metadata": {},
   "source": [
    "#### Baseline model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4059934",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_baseline, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = score_dataset(dataset_path_eval_answer_baseline, dataset_path_eval_answer_score_baseline, dataset_path_eval_answer_score_metrics_baseline)\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(baseline_result[\"metrics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "studio_url = baseline_result[\"studio_url\"] or \"http://127.0.0.1:23333\"\n",
    "print(f\"Results available at {studio_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404ec34",
   "metadata": {},
   "source": [
    "#### Student model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185474",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_student, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_result = score_dataset(dataset_path_eval_answer_student, dataset_path_eval_answer_score_student, dataset_path_eval_answer_score_metrics_student)\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(student_result[\"metrics\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "studio_url = student_result[\"studio_url\"] or \"http://127.0.0.1:23333\"\n",
    "print(f\"Results available at {studio_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78572d",
   "metadata": {},
   "source": [
    "## Let's look at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline=pd.read_json(dataset_path_eval_answer_score_baseline, lines=True)\n",
    "df_student=pd.read_json(dataset_path_eval_answer_score_student, lines=True)\n",
    "df_merged=pd.merge(df_baseline, df_student, on=\"inputs.question\", suffixes=('_baseline', '_student'))\n",
    "df_merged.insert(0, \"id\", df_merged.index)\n",
    "df_merged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79181e85",
   "metadata": {},
   "source": [
    "## Compare the metrics of the student model against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42af516-f4b7-4f25-b15b-791d0a9c93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    import json\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "student_metrics = read_json(dataset_path_eval_answer_score_metrics_student)\n",
    "baseline_metrics = read_json(dataset_path_eval_answer_score_metrics_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame.from_dict({\"baseline\": baseline_metrics, \"student\": student_metrics})\n",
    "metrics[\"improvement\"] = (metrics[\"student\"] - metrics[\"baseline\"]) / metrics[\"baseline\"]\n",
    "gpt_metric_names = set(filter(lambda e: 'gpt' in e, metrics.index.values))\n",
    "gpt_mask = metrics.index.isin(gpt_metric_names)\n",
    "metrics_gpt = metrics[gpt_mask] # between 1 and 5\n",
    "metrics_math = metrics[~gpt_mask] # between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f50d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9078ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define subplot layout\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(7, 7))\n",
    "axes[0].set_title('Math metrics')\n",
    "metrics_math.drop(\"improvement\", axis=1).plot.barh(rot=0, colormap='Dark2', ax=axes[0])\n",
    "axes[1].set_title('GPT metrics')\n",
    "metrics_gpt.drop(\"improvement\", axis=1).plot.barh(rot=0, colormap='Dark2', ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define subplot layout\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(7, 7))\n",
    "axes[0].set_title('Math metrics')\n",
    "metrics_math[\"improvement\"].plot.barh(rot=0, ax=axes[0], color=(metrics_math[\"improvement\"] > 0).map({True: 'g', False: 'r'}))\n",
    "axes[1].set_title('GPT metrics')\n",
    "metrics_gpt[\"improvement\"].plot.barh(rot=0, ax=axes[1], color=(metrics_gpt[\"improvement\"] > 0).map({True: 'g', False: 'r'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d127e5",
   "metadata": {},
   "source": [
    "## Let's look at outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a20d5-88d1-45db-a137-d62d149a0a98",
   "metadata": {},
   "source": [
    "### Compute improvement for each sample and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes=[column.replace(\"_baseline\", \"\").replace(\"outputs.\", \"\") for column in df_merged.columns if column.startswith(\"outputs.\") and column.endswith(\"_baseline\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improvements = df_merged.copy()\n",
    "for suffixe in suffixes:\n",
    "    df_improvements[f\"improvement_outputs.{suffixe}\"] = (df_improvements[f\"outputs.{suffixe}_student\"] - df_improvements[f\"outputs.{suffixe}_baseline\"]) / df_improvements[f\"outputs.{suffixe}_baseline\"]\n",
    "df_improvements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf84af-09e3-40f3-9590-e224dde6206d",
   "metadata": {},
   "source": [
    "### Find samples for the worst GPT Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc984c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_columns=['improvement_outputs.fluency.gpt_fluency']\n",
    "display_columns=[\"id\", \"inputs.question\", \"inputs.final_answer_baseline\", \"inputs.final_answer_student\", \"improvement_outputs.fluency.gpt_fluency\", \"inputs.gold_final_answer_student\"]\n",
    "df_improvements.sort_values(by=sort_columns, ascending=True)[display_columns].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd589a-fac4-41c3-99fb-4e22564069c7",
   "metadata": {},
   "source": [
    "### Find samples for the best GPT Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improvements.sort_values(by=sort_columns, ascending=False)[display_columns].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c2e4b-e678-4bc6-8cfb-a178f2c159ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
