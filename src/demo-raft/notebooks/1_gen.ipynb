{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic dataset generation using RAFT\n",
    "\n",
    "This recipe will walk you through using a big LLM such as [OpenAI GPT-4o](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure#gpt-4o-and-gpt-4-turbo) or [Meta Llama 3.1 405B](https://aka.ms/c/model/Meta-Llama-3.1-405B-Instruct) deployed on Azure AI to generate a synthetic dataset using UC Berkeley's Gorilla project RAFT method (see [blog post](https://aka.ms/raft-blog))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "![](./doc/raft-process-gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAFT?\n",
    "\n",
    "RAFT stands for Retrieval Augmented Fine Tuning. The general principle is to use a big LLM such as Llama 3.1 405B to analyse a set of documents and generate a dataset of questions and answers that users might want to ask about those documents. We can then use that QA dataset to fine tune a smaller model such as Llama 3.1 8B. The fine tune model will therefore be better at answering questions about those documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy: How to prepare a LLM for an Exam? üìù\n",
    "\n",
    "*Note: This description was copied from the [UC Berkeley RAFT blog post](https://aka.ms/raft-blog-ucb).*\n",
    "\n",
    "RAFT is a general recipe to finetune a pretrained LLM to your domain-specific RAG settings. This is a common scenario where you want your LLM to answer questions grounded on a set of documents, for e.g., private files in an enterprise. Such a setting is different from the general RAG where the LLM does not know which domain (of documents) it will be tested on. To better illustrate this setting, let's draw an analogy between deploying and using an LLM with the real-world setting of prepararing for an exam.\n",
    "\n",
    "![RAFT Open book principle](./doc/raft_openbook.png \"RAFT Open book principle\")\n",
    "\n",
    "#### Closed-Book Exam\n",
    "\n",
    "A closed book exam often refers to the scenario where the LLMs do not have access to any additional documents or references to answer the questions during the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is used as a chatbot. In this scenario the LLM draws from the knowledge baked in during pre-training and supervised-finetuning to respond to the users' prompt.\n",
    "\n",
    "#### Open-Book Exam\n",
    "\n",
    "In contrast, we liken the open-book exam setting to the scenario in which the LLM can refer to external sources of information (e.g., a website or a book chapter). In such scenarios, typically, the LLM is paired with retriever which retrieves k documents (or specific segments of the document) which are appended to the users' prompt. It is only through these documents retrieved that the LLM gains access to new knowledge. As a result, we argue that the LLM's performance in these settings, where it is trained as a general-purpose LLM is largely dependent on the quality of the retriever and how accurately the retriever can identify the most relevant piece of information.\n",
    "\n",
    "#### RAFT\n",
    "\n",
    "RAFT focuses on a narrower but increasingly popular domain than the general open book exam, called the domain-specific open-book exam. In domain-specific open book exam, we know a priori the domain in which the LLM will be tested --- used for inference. The LLM can respond to the users' prompt using use any and all information from this specific domain, which it has been fine-tuned on. Examples of domain specific examples include enterprise documents, latest news, code repositories belonging to an organization, etc. In all these scenarios, the LLM will be used to respond to the questions, whose answers can be found within a collection of documents (a small practical domain). The retrieval technique itself has little to no-impact on the mechanism (though it may impact the accuracy). This paper mainly studies this, domain-specific open-book setting and how to adapt a pretrained LLM to this specific domain, including how to make it more robust to a varying number of retrieved documents and distractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAFT Process: from domain documents to Q/A/CoT dataset splits\n",
    "\n",
    "The process is the following. RAFT takes as input a set of documents, split them into chunks, and for each chunk generates a list of questions, Chain Of Thought answers with a selection of relevant and irrelevant context chunks.\n",
    "\n",
    "![RAFT](./doc/raft.png \"RAFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running time and cost\n",
    "\n",
    "The RAFT script usually takes a few minutes on the default sample document but can take days on bigger domains depending on the number and size of documents and the number of questions being generated for each chunk.\n",
    "\n",
    "The cost of running this RAFT script on the sample document should be a few dollars. But beware, running it on bigger domains can cost hundreds of dollars if not more. It is safe to run this notebook multiple times though as the costly part, running the `raft.py` script, will only be executed if the dataset doesn't exist yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Before running this notebook, let's make sure your environment is ready\n",
    "\n",
    "### 1. Deploy Meta Llama 3.1 405B Instruct as a serverless endpoint.\n",
    "\n",
    "This model will be used to generate the synthetic dataset.\n",
    "\n",
    "You can either use [Azure ML Studio](https://aka.ms/raft-llama-31-learn-deploy-405b) or [Azure AI Studio](https://aka.ms/raft-llama-31-learn-deploy-405b-ai-studio).\n",
    "\n",
    "**Note**: an Azure ML Workspace is the same as a Azure AI Hub, you will be able to go back and forth between the two transparently.\n",
    "\n",
    "### 2. Deploy OpenAI's `text-embedding-ada-002` as a serverless endpoint.\n",
    "\n",
    "This model will be used to create the chunk embeddings.\n",
    "\n",
    "You can follow the same procedure as for the Meta Llama model deployment\n",
    "\n",
    "### 3. Setup your environment variables\n",
    "\n",
    "Copy the `.env.sample` file to `.env` and update according to your Azure AI project configuration and deployed endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAFT repository\n",
    " \n",
    "This script will checkout a shallow and narrow clone of the UC Berkeley Gorilla RAFT repository locally so that this notebook can invoke the RAFT script and util functions. It can safely be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./setup_raft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "\n",
    "The requirements should have been automatically installed if you opened the project in Dev Container or Codespaces, but if not, uncomment the following cell to install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check instrastructure requirements\n",
    "\n",
    "The following integration tests check that the endpoints are accessible and working before executing the RAFT program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pytest --rootdir=infra/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load infrastructure environnment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Variables passed by previous notebooks\n",
    "load_dotenv(\".env.state\")\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "EMBEDDING_DEPLOYMENT_NAME=getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
    "TEACHER_DEPLOYMENT_NAME=getenv(\"TEACHER_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(f\"Using embedding model {EMBEDDING_DEPLOYMENT_NAME}\")\n",
    "print(f\"Using teacher model {TEACHER_DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook parameters\n",
    "\n",
    "*Note: Parameters are typed as indicated for Papermill introspection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ds_name: str = \"zava\"\n",
    "doc_path: str = \"../manuals/\"\n",
    "format: str = \"chat\"\n",
    "finetuning_train_split : int = .8\n",
    "finetuning_valid_split : int = .1\n",
    "finetuning_threshold : int = 65\n",
    "raft_questions: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import update_state\n",
    "\n",
    "ds_path = f\"dataset/{ds_name}\"\n",
    "ds_output_file = f\"{ds_path}.jsonl\"\n",
    "update_state(\"DATASET_NAME\", ds_name)\n",
    "print(\"Creating dataset: \" + ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $ds_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pdf_image\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_image = None\n",
    "if Path(doc_path).exists() and Path(doc_path).is_file() and Path(doc_path).suffix == \".pdf\":\n",
    "    pdf_image = get_pdf_image(doc_path)\n",
    "pdf_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Q/A/CoT fine-tuning dataset using RAFT from the domain specific documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--completion_model` and `--embedding_model` parameters refer to the names of the deployments of the models in Azure.\n",
    "\n",
    "**Note about `--qa-threshold`**: The Azure AI Finetuning service requires a minimum of 65 samples in the training split so in order to make this notebook run as quickly as possible with a demo dataset, we calculate the minimum number of samples we need to generate overall using the `finetuning_threshold` and the `finetuning_train_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "qa_threshold = ceil(finetuning_threshold / finetuning_train_split)\n",
    "print(f\"QA threshold: {qa_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch .env\n",
    "! env $(cat .env .env.state) python3 .gorilla/raft/raft.py \\\n",
    "    --datapath \"$doc_path\" \\\n",
    "    --output $ds_path \\\n",
    "    --distractors 3 \\\n",
    "    --doctype pdf \\\n",
    "    --chunk_size 512 \\\n",
    "    --questions $raft_questions \\\n",
    "    --workers 2 \\\n",
    "    --system-prompt-key gpt \\\n",
    "    --completion_model $TEACHER_DEPLOYMENT_NAME \\\n",
    "    --embedding_model $EMBEDDING_DEPLOYMENT_NAME \\\n",
    "    --qa-threshold $qa_threshold \\\n",
    "    --completion-env-prefix TEACHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The bit of shell logic wrapping the python script call allows to skip the generation if the dataset has already been generated so it is safe to run this notebook multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training, validation and evaluation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define variables for the different files we will need throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raft_arrow_file = f\"{ds_path}/data-00000-of-00001.arrow\"\n",
    "dataset_path = f\"{ds_path}-files/{ds_name}-full.jsonl\"\n",
    "dataset_path_hf = f\"{ds_path}-files/{ds_name}-hf.full.jsonl\"\n",
    "\n",
    "dataset_path_hf_train = f\"{ds_path}-files/{ds_name}-hf.train.jsonl\"\n",
    "dataset_path_hf_valid = f\"{ds_path}-files/{ds_name}-hf.valid.jsonl\"\n",
    "dataset_path_hf_eval = f\"{ds_path}-files/{ds_name}-hf.eval.jsonl\"\n",
    "\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "\n",
    "print(f\"Reading arrow file {raft_arrow_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset to JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export the Apache Arrow format file to JSONL, easier to manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $raft_arrow_file \\\n",
    "    --output $dataset_path_hf \\\n",
    "    --output-format hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_full_df = pd.read_json(dataset_path_hf, lines=True)\n",
    "hf_full_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "\n",
    "sample_idx = 2#randint(0, len(hf_full_df) - 1)\n",
    "sample = hf_full_df.iloc[sample_idx]\n",
    "instruction_md = sample.instruction.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "oracle_context_md = sample.oracle_context.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "sample_answer_md = sample.cot_answer.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "display(Markdown(f\"\"\"\n",
    "## Oracle Context\n",
    "{oracle_context_md}\n",
    "\n",
    "## Question\n",
    "{sample.question}\n",
    "\n",
    "## CoT Answer\n",
    "{sample_answer_md}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train / validation / evaluation\n",
    "\n",
    "In machine learning, splitting a dataset into training, validation, and test sets is a fundamental step to ensure that your model is trained effectively, evaluated properly, and generalizes well to new data. Here‚Äôs a brief explanation of each split:\n",
    "\n",
    "- **Training Split**: The training set (80% of the data) is used to train the model. It helps the model learn patterns and relationships by adjusting its internal parameters based on input-output pairs.\n",
    "\n",
    "- **Validation Split**: The validation set (10%) is used during training to monitor the model‚Äôs performance and guide convergence. It helps fine-tune hyperparameters and ensures the model doesn‚Äôt overfit by providing feedback on unseen data during training.\n",
    "\n",
    "- **Evaluation Split (sometimes also called test split)**: The test set (10%) is used only after training is complete to evaluate the model‚Äôs final performance. It provides an unbiased measure of how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 80%/10%/10%\n",
    "import numpy as np\n",
    "\n",
    "samples_count = len(hf_full_df)\n",
    "splits = [int(finetuning_train_split * samples_count), int((finetuning_train_split + finetuning_valid_split) * samples_count)]\n",
    "print(f\"Splitting dataset at {splits}\")\n",
    "hf_train_df, hf_valid_df, hf_eval_df = np.split(hf_full_df, splits)\n",
    "hf_train_df.to_json(dataset_path_hf_train, orient=\"records\", lines=True)\n",
    "hf_valid_df.to_json(dataset_path_hf_valid, orient=\"records\", lines=True)\n",
    "hf_eval_df.to_json(dataset_path_hf_eval, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export training and validation splits into JSONL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_train \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_train \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_valid \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_valid \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_ft_valid_df = pd.read_json(dataset_path_ft_valid, lines=True)\n",
    "dataset_path_ft_valid_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the evaluation split aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to format the evaluation dataset for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_hf_eval, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step -> Fine-tuning\n",
    "\n",
    "- [./2_finetune_oai.ipynb](./2_finetune_oai.ipynb) if fine-tuning an **OpenAI** student model such as GPT-4o-mini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
