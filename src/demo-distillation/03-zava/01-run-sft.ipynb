{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18b4e33",
   "metadata": {},
   "source": [
    "# Supervised Fine Tuning with GPT-4.1\n",
    "\n",
    "**See**: [Documentation](https://learn.microsoft.com/azure/ai-foundry/openai/tutorials/fine-tune?tabs=bash#create-a-sample-dataset)\n",
    "\n",
    "This tutorial teaches you the basic workflow for fine-tuning a base model (gpt-4.1) using supervised fine tuning. \n",
    "The workflow involves the following steps:\n",
    "\n",
    "- Create sample fine-tuning datasets.\n",
    "- Create environment variables for our resource endpoint and API key.\n",
    "- Prepare your sample training and validation datasets for fine-tuning.\n",
    "- Upload your training file and validation file for fine-tuning.\n",
    "- Create a fine-tuning job for gpt-4o-mini-2024-07-18.\n",
    "- Deploy a custom fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8a41e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Read Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6be44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: gpt-4.1\n",
      "Using API Version: 2025-02-01-preview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = os.getenv(\"DEMO_BASIC_MODEL\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cff04b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Validate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fb1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set File names once\n",
    "training_file_name = '01-training_set.jsonl'\n",
    "validation_file_name = '01-validation_set.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d806159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in training set: 20\n",
      "First example in training set:\n",
      "{'role': 'system', 'content': 'Cora is a polite and cheerful chatbot that offers helpful suggestions on Zava products for home improvement!'}\n",
      "{'role': 'user', 'content': 'What brush could I use for oil-based paints?'}\n",
      "{'role': 'assistant', 'content': \"Excellent question! I'd recommend our Natural Bristle Brush Set (PFBR000017) for $23. These traditional natural bristle brushes are specifically designed for oil-based paints and stains, providing superior flow and leveling for a professional finish. They also pair perfectly with our Universal Bonding Primer for all surfaces. Do you want to learn more, or can I help you with a different question?\"}\n",
      "\n",
      "Number of examples in validation set: 20\n",
      "First example in validation set:\n",
      "{'role': 'system', 'content': 'Cora is a polite and cheerful chatbot that offers helpful suggestions on Zava products for home improvement!'}\n",
      "{'role': 'user', 'content': 'I need a paint tray for my roller project.'}\n",
      "{'role': 'assistant', 'content': 'Wonderful choice for organized painting! Our Metal Paint Tray 9-inch (PFPT000042) for $10 is perfect with its durable metal construction, deep well, and ribbed ramp for proper roller loading. You might also like our Paint Tray Liner Set for quick cleanup and easy color changes. Do you want to learn more about painting techniques, or can I help you with something else?'}\n"
     ]
    }
   ],
   "source": [
    "# Run preliminary checks\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the training set\n",
    "with open(training_file_name, 'r', encoding='utf-8') as f:\n",
    "    training_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Training dataset stats\n",
    "print(\"Number of examples in training set:\", len(training_dataset))\n",
    "print(\"First example in training set:\")\n",
    "for message in training_dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Load the validation set\n",
    "with open(validation_file_name, 'r', encoding='utf-8') as f:\n",
    "    validation_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Validation dataset stats\n",
    "print(\"\\nNumber of examples in validation set:\", len(validation_dataset))\n",
    "print(\"First example in validation set:\")\n",
    "for message in validation_dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac5cb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Validate Token Counts (with TikToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421a3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 01-training_set.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 113, 138\n",
      "mean / median: 125.65, 125.5\n",
      "p5 / p95: 118.7, 132.3\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 71, 89\n",
      "mean / median: 80.5, 81.0\n",
      "p5 / p95: 74.8, 85.1\n",
      "**************************************************\n",
      "Processing file: 01-validation_set.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 112, 127\n",
      "mean / median: 121.0, 121.5\n",
      "p5 / p95: 115.0, 125.1\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 69, 84\n",
      "mean / median: 77.1, 77.0\n",
      "p5 / p95: 71.9, 81.1\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Validate token counts\n",
    "\n",
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\") # default encoding for gpt-4o models. This requires the latest version of tiktoken to be installed.\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "files = [training_file_name, validation_file_name]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    total_tokens = []\n",
    "    assistant_tokens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex.get(\"messages\", {})\n",
    "        total_tokens.append(num_tokens_from_messages(messages))\n",
    "        assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print_distribution(total_tokens, \"total tokens\")\n",
    "    print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6d47e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Upload Fine Tuning Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca2ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-ec5c778cea254382b88d41680ca4d134\n",
      "Validation file ID: file-c97252cb6cbc436cb9f431393f756012\n"
     ]
    }
   ],
   "source": [
    "# Upload fine-tuning files\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "  api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "\n",
    "training_response = client.files.create(\n",
    "    file = open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file = open(validation_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfd8ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Begin Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03603de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-499afbe45d7444bb87740de97ea6f650\n",
      "Status: pending\n",
      "{\n",
      "  \"id\": \"ftjob-499afbe45d7444bb87740de97ea6f650\",\n",
      "  \"created_at\": 1754878180,\n",
      "  \"error\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"hyperparameters\": {\n",
      "    \"batch_size\": -1,\n",
      "    \"learning_rate_multiplier\": 2.0,\n",
      "    \"n_epochs\": -1\n",
      "  },\n",
      "  \"model\": \"gpt-4.1-2025-04-14\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": null,\n",
      "  \"result_files\": null,\n",
      "  \"seed\": 101,\n",
      "  \"status\": \"pending\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-ec5c778cea254382b88d41680ca4d134\",\n",
      "  \"validation_file\": \"file-c97252cb6cbc436cb9f431393f756012\",\n",
      "  \"estimated_finish\": 1754879260,\n",
      "  \"integrations\": null,\n",
      "  \"metadata\": null,\n",
      "  \"method\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Submit fine-tuning training job\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file = training_file_id,\n",
    "    validation_file = validation_file_id,\n",
    "    model = model_name, # Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters.\n",
    "    seed = 101 # 105 # seed parameter controls reproducibility of the fine-tuning job. If no seed is specified one will be generated automatically.\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eabcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3b28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Track Training Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training status\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the status of our fine-tuning job.\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "status = response.status\n",
    "\n",
    "# If the job isn't done yet, poll it every 10 seconds.\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)\n",
    "\n",
    "    response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    print(response.model_dump_json(indent=2))\n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "    status = response.status\n",
    "    print(f'Status: {status}')\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f'Fine-tuning job {job_id} finished with status: {status}')\n",
    "\n",
    "# List all fine-tuning jobs for this resource.\n",
    "print('Checking other fine-tune jobs for this resource.')\n",
    "response = client.fine_tuning.jobs.list()\n",
    "print(f'Found {len(response.data)} fine-tune jobs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9e7e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. List Fine-Tuning Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb103182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd8829f832a808ddd8829f832a80\",\n",
      "      \"created_at\": 1754880969,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 100: training loss=0.8421152830123901\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 100,\n",
      "        \"train_loss\": 0.8421152830123901,\n",
      "        \"train_mean_token_accuracy\": 0.7435897588729858,\n",
      "        \"valid_loss\": 1.5004924339584158,\n",
      "        \"valid_mean_token_accuracy\": 0.6582278481012658,\n",
      "        \"full_valid_loss\": 1.154685450866159,\n",
      "        \"full_valid_mean_token_accuracy\": 0.7035398230088495\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd882998d49808ddd882998d4980\",\n",
      "      \"created_at\": 1754880959,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 90: training loss=0.4213077425956726\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 90,\n",
      "        \"train_loss\": 0.4213077425956726,\n",
      "        \"train_mean_token_accuracy\": 0.8500000238418579,\n",
      "        \"valid_loss\": 1.1470455115949605,\n",
      "        \"valid_mean_token_accuracy\": 0.7323943661971831\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd882939768808ddd88293976880\",\n",
      "      \"created_at\": 1754880949,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 80: training loss=0.46103110909461975\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 80,\n",
      "        \"train_loss\": 0.46103110909461975,\n",
      "        \"train_mean_token_accuracy\": 0.8620689511299133,\n",
      "        \"valid_loss\": 1.0581457676031651,\n",
      "        \"valid_mean_token_accuracy\": 0.7692307692307693,\n",
      "        \"full_valid_loss\": 1.1585619826383144,\n",
      "        \"full_valid_mean_token_accuracy\": 0.706700379266751\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd8828da187808ddd8828da18780\",\n",
      "      \"created_at\": 1754880939,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 70: training loss=0.7718716263771057\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 70,\n",
      "        \"train_loss\": 0.7718716263771057,\n",
      "        \"train_mean_token_accuracy\": 0.759036123752594,\n",
      "        \"valid_loss\": 0.9793501132872047,\n",
      "        \"valid_mean_token_accuracy\": 0.7560975609756098\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd88287aba6808ddd88287aba680\",\n",
      "      \"created_at\": 1754880929,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 60: training loss=0.7837918996810913\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 60,\n",
      "        \"train_loss\": 0.7837918996810913,\n",
      "        \"train_mean_token_accuracy\": 0.7317073345184326,\n",
      "        \"valid_loss\": 1.1752675129817083,\n",
      "        \"valid_mean_token_accuracy\": 0.6666666666666666,\n",
      "        \"full_valid_loss\": 1.1865328657340761,\n",
      "        \"full_valid_mean_token_accuracy\": 0.690897597977244\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd88281b5c5808ddd88281b5c580\",\n",
      "      \"created_at\": 1754880919,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 50: training loss=1.2979933023452759\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 50,\n",
      "        \"train_loss\": 1.2979933023452759,\n",
      "        \"train_mean_token_accuracy\": 0.6623376607894897,\n",
      "        \"valid_loss\": 1.1601489827602725,\n",
      "        \"valid_mean_token_accuracy\": 0.6582278481012658\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd8827bbfe4808ddd8827bbfe480\",\n",
      "      \"created_at\": 1754880909,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 40: training loss=0.9554105997085571\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 40,\n",
      "        \"train_loss\": 0.9554105997085571,\n",
      "        \"train_mean_token_accuracy\": 0.7586206793785095,\n",
      "        \"valid_loss\": 1.7899597406387329,\n",
      "        \"valid_mean_token_accuracy\": 0.5625,\n",
      "        \"full_valid_loss\": 1.349777429052611,\n",
      "        \"full_valid_mean_token_accuracy\": 0.6573957016434893\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd88275ca03808ddd88275ca0380\",\n",
      "      \"created_at\": 1754880899,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 30: training loss=1.7568920850753784\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 30,\n",
      "        \"train_loss\": 1.7568920850753784,\n",
      "        \"train_mean_token_accuracy\": 0.5641025900840759,\n",
      "        \"valid_loss\": 1.786624763585344,\n",
      "        \"valid_mean_token_accuracy\": 0.5822784810126582\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd8826fd422808ddd8826fd42280\",\n",
      "      \"created_at\": 1754880889,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 20: training loss=1.9777320623397827\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 20,\n",
      "        \"train_loss\": 1.9777320623397827,\n",
      "        \"train_mean_token_accuracy\": 0.5,\n",
      "        \"valid_loss\": 1.9830132997952974,\n",
      "        \"valid_mean_token_accuracy\": 0.41025641025641024,\n",
      "        \"full_valid_loss\": 1.9531169679161873,\n",
      "        \"full_valid_mean_token_accuracy\": 0.5075853350189633\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftevent-808ddd88269de41808ddd88269de4180\",\n",
      "      \"created_at\": 1754880879,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 10: training loss=2.393355131149292\",\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"data\": {\n",
      "        \"step\": 10,\n",
      "        \"train_loss\": 2.393355131149292,\n",
      "        \"train_mean_token_accuracy\": 0.5058823823928833,\n",
      "        \"valid_loss\": 2.5868834142815578,\n",
      "        \"valid_mean_token_accuracy\": 0.4657534246575342\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": true,\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442ad53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. List Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c89c9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"ftchkpt-ce9351da53894ed493e709715a53ec19\",\n",
      "      \"created_at\": 1754883765,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-499afbe45d7444bb87740de97ea6f650\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-499afbe45d7444bb87740de97ea6f650\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 1.154685450866159,\n",
      "        \"full_valid_mean_token_accuracy\": 0.7035398230088495,\n",
      "        \"step\": 100.0,\n",
      "        \"train_loss\": 0.8421152830123901,\n",
      "        \"train_mean_token_accuracy\": 0.7435897588729858,\n",
      "        \"valid_loss\": 1.5004924339584158,\n",
      "        \"valid_mean_token_accuracy\": 0.6582278481012658\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 100\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftchkpt-e4bca86caaee4fe1977ca41ce9f790f2\",\n",
      "      \"created_at\": 1754883204,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-499afbe45d7444bb87740de97ea6f650:ckpt-step-80\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-499afbe45d7444bb87740de97ea6f650\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 1.1585619826383144,\n",
      "        \"full_valid_mean_token_accuracy\": 0.706700379266751,\n",
      "        \"step\": 80.0,\n",
      "        \"train_loss\": 0.46103110909461975,\n",
      "        \"train_mean_token_accuracy\": 0.8620689511299133,\n",
      "        \"valid_loss\": 1.0581457676031651,\n",
      "        \"valid_mean_token_accuracy\": 0.7692307692307693\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 80\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ftchkpt-4cbc781955d54fb29e63c3849a5215dc\",\n",
      "      \"created_at\": 1754882636,\n",
      "      \"fine_tuned_model_checkpoint\": \"gpt-4.1-2025-04-14.ft-499afbe45d7444bb87740de97ea6f650:ckpt-step-60\",\n",
      "      \"fine_tuning_job_id\": \"ftjob-499afbe45d7444bb87740de97ea6f650\",\n",
      "      \"metrics\": {\n",
      "        \"full_valid_loss\": 1.1865328657340761,\n",
      "        \"full_valid_mean_token_accuracy\": 0.690897597977244,\n",
      "        \"step\": 60.0,\n",
      "        \"train_loss\": 0.7837918996810913,\n",
      "        \"train_mean_token_accuracy\": 0.7317073345184326,\n",
      "        \"valid_loss\": 1.1752675129817083,\n",
      "        \"valid_mean_token_accuracy\": 0.6666666666666666\n",
      "      },\n",
      "      \"object\": \"fine_tuning.job.checkpoint\",\n",
      "      \"step_number\": 60\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": false,\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.checkpoints.list(job_id)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a4752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Final Training Run Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e2d17ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"ftjob-499afbe45d7444bb87740de97ea6f650\",\n",
      "  \"created_at\": 1754878180,\n",
      "  \"error\": null,\n",
      "  \"fine_tuned_model\": \"gpt-4.1-2025-04-14.ft-499afbe45d7444bb87740de97ea6f650\",\n",
      "  \"finished_at\": 1754886012,\n",
      "  \"hyperparameters\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 2.0,\n",
      "    \"n_epochs\": 5\n",
      "  },\n",
      "  \"model\": \"gpt-4.1-2025-04-14\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": null,\n",
      "  \"result_files\": [\n",
      "    \"file-f2bf0b22627c46aa8285142139f5d56c\"\n",
      "  ],\n",
      "  \"seed\": 101,\n",
      "  \"status\": \"succeeded\",\n",
      "  \"trained_tokens\": 16665,\n",
      "  \"training_file\": \"file-ec5c778cea254382b88d41680ca4d134\",\n",
      "  \"validation_file\": \"file-c97252cb6cbc436cb9f431393f756012\",\n",
      "  \"estimated_finish\": 1754880457,\n",
      "  \"integrations\": null,\n",
      "  \"metadata\": null,\n",
      "  \"method\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine_tuned_model name\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "fine_tuned_model = response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716a1fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Deploy Fine-Tuned Model\n",
    "\n",
    "Follow the new guidance [at this document](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tune-test?tabs=portal) to learn how to use the _developer tier_ for more cost effective testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........... COMMENTED OUT FOR NOW - DEPLOYMENT DONE MANUALLY VIA PORTAL ......\n",
    "\n",
    "'''\n",
    "# Deploy fine-tuned model\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "token = os.getenv(\"TEMP_AUTH_TOKEN\")\n",
    "subscription = \"<YOUR_SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<YOUR_RESOURCE_GROUP_NAME>\"\n",
    "resource_name = \"<YOUR_AZURE_OPENAI_RESOURCE_NAME>\"\n",
    "model_deployment_name = \"gpt-4o-mini-2024-07-18-ft\" # Custom deployment name you chose for your fine-tuning model\n",
    "\n",
    "deploy_params = {'api-version': \"2024-10-01\"} # Control plane API version\n",
    "deploy_headers = {'Authorization': 'Bearer {}'.format(token), 'Content-Type': 'application/json'}\n",
    "\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"standard\", \"capacity\": 1},\n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": \"<YOUR_FINE_TUNED_MODEL>\", #retrieve this value from the previous call, it will look like gpt-4o-mini-2024-07-18.ft-0e208cf33a6a466994aff31a08aba678\n",
    "            \"version\": \"1\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "request_url = f'https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}'\n",
    "\n",
    "print('Creating a new deployment...')\n",
    "\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80778a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........... COMMENTED OUT FOR NOW - DEPLOYMENT DONE MANUALLY VIA PORTAL ......\n",
    "'''\n",
    "# Deploying with Developer Tier\n",
    "#\n",
    "# to obtain the TOKEN parameter, simply access the Cloud Shell in the Azure portal \n",
    "# and execute the az account get-access-token command. This will generate the \n",
    "# necessary authorization token for your deployment tasks, making the process \n",
    "# efficient and straightforward.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "token = os.getenv(\"<TOKEN>\") \n",
    "subscription = \"<YOUR_SUBSCRIPTION_ID>\"  \n",
    "resource_group = \"<YOUR_RESOURCE_GROUP_NAME>\"\n",
    "resource_name = \"<YOUR_AZURE_OPENAI_RESOURCE_NAME>\"\n",
    "model_deployment_name = \"gpt41-mini-candidate-01\" # custom deployment name that you will use to reference the model when making inference calls.\n",
    "\n",
    "deploy_params = {'api-version': \"2025-04-01-preview\"} \n",
    "deploy_headers = {'Authorization': 'Bearer {}'.format(token), 'Content-Type': 'application/json'}\n",
    "\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"developertier\", \"capacity\": 50},\n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": <\"fine_tuned_model\">, #retrieve this value from the previous call, it will look like gpt41-mini-candidate-01.ft-b044a9d3cf9c4228b5d393567f693b83\n",
    "            \"version\": \"1\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "request_url = f'https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}'\n",
    "\n",
    "print('Creating a new deployment...')\n",
    "\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fd90c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Use Deployed Customized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........... COMMENTED OUT FOR NOW - TESTING DONE MANUALLY VIA PORTAL ......\n",
    "\n",
    "'''\n",
    "\n",
    "# Use the deployed customized model\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "  api_version = \"2024-10-21\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini-2024-07-18-ft\", # model = \"Custom deployment name you chose for your fine-tuning model\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure services support this too?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a528d03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. DELETE DEPLOYMENT‼️\n",
    "\n",
    "> [Developer Deployments](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tune-test?tabs=portal#clean-up-your-deployment) will delete on their own after the 24-hour default window. For all others, delete manually.\n",
    "\n",
    "\n",
    "Unlike other types of Azure OpenAI models, fine-tuned/customized models have an hourly hosting cost associated with them once they're deployed. It's strongly recommended that once you're done with this tutorial and have tested a few chat completion calls against your fine-tuned model, that you delete the model deployment.\n",
    "\n",
    "Use this command from CLI to delete the deployed model where the placeholder variables should be replaced with the values for your deployment.\n",
    "\n",
    "```bash\n",
    "curl -X DELETE \"https://management.azure.com/subscriptions/<SUBSCRIPTION>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.CognitiveServices/accounts/<RESOURCE_NAME>/deployments/<MODEL_DEPLOYMENT_NAME>api-version=2025-04-01-preview\" \\\n",
    "  -H \"Authorization: Bearer <TOKEN>\"\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
