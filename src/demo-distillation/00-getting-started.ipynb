{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b9b1da",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">üîç | Step 1: Understand Zava Scenario</h1>\n",
    "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "        By now you have setup your infrastructure and have a valid Azure AI Foundry project with required models deployed. You've created an Azure AI Search resource with a zava-products index containing 50 products. And you can connected the local development environment to your provisioned backend by using Azure CLI login to establish credentials, and updating the .env file to configure local variables. <b> It's time to look at the data and begin our model customization journey! </b>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f640e1",
   "metadata": {},
   "source": [
    "## 0. Add More Models\n",
    "\n",
    "This is the start of \"Act 2\" where we look at the Fine-Tuning options available in Azure AI Foundry. For his section, let's deploy a few additional models that we will be using over the next couple of demos, to get an intuitive sense for how to <em>pick</em> the best starting model or our needs - then <em>customize</em> it to provide the required optimization - and <em>evaluate</em> it to see if it has improved over the base model we started with.\n",
    "\n",
    "For now, just add these models using the Azure AI Foundry Portal UI\n",
    "1. Visit the Azure AI Foundry portal - and open your project page\n",
    "1. Select the Models + Endpoints tab - in the left sidebar\n",
    "1. Deploy the required models - till you have this specific set:\n",
    "    - Reasoning Models ‚Üí o3, o3-mini, o4-mini\n",
    "    - Chat Models ‚Üí  gpt-4o, gpt-4.1, gpt-4.1-nano\n",
    "    - Embedding Models ‚Üí  text-embedding-ada-002\n",
    "\n",
    "Your `.env` variables should already be set to the required Azure OpenAI endpoint - we are ready to go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5e58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Setup an Azure OpenAI client and test out different models\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-05-01-preview\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396a202",
   "metadata": {},
   "source": [
    "### 1. Define Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25becf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Define a test prompt that we'll use for all models\n",
    "test_prompt = \"\"\"\n",
    "You are a home improvement assistant for Zava, a fictional hardware store.\n",
    "Please give me a brief recommendation for a paint color for my living room.\n",
    "Include one key feature of the paint and a price range.\n",
    "\"\"\"\n",
    "\n",
    "# List of models to test\n",
    "models_to_test = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"o3\",\n",
    "    #\"o3-mini\",\n",
    "    #\"o4-mini\"\n",
    "]\n",
    "\n",
    "# Function to call a model and measure performance\n",
    "def test_model(model_name, prompt):\n",
    "    print(f\"..... Testing model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add the appropriate token limit parameter based on model type\n",
    "        if model_name.startswith(\"o\"):\n",
    "            params[\"max_completion_tokens\"] = 300\n",
    "        else:\n",
    "            params[\"max_tokens\"] = 300\n",
    "        \n",
    "        response = client.chat.completions.create(**params)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        \n",
    "        # Extract response and token usage\n",
    "        content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"latency\": latency,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"response\": content\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with model {model_name}: {str(e)}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f61e2",
   "metadata": {},
   "source": [
    "### 2. Run Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372c618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... Testing model: gpt-4o\n",
      "..... Testing model: gpt-4.1\n",
      "..... Testing model: gpt-4.1-nano\n",
      "..... Testing model: o3\n"
     ]
    }
   ],
   "source": [
    "# Test each model with the same prompt\n",
    "results = []\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        result = test_model(model, test_prompt)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception outside test_model for {model}: {str(e)}\")\n",
    "        results.append({\"model\": model, \"error\": str(e)})\n",
    "\n",
    "# Store the detailed output for later, but don't display all of it \n",
    "detailed_outputs = {}\n",
    "for result in results:\n",
    "    if \"error\" not in result:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": result[\"response\"],\n",
    "            \"latency\": result[\"latency\"],\n",
    "            \"prompt_tokens\": result[\"prompt_tokens\"],\n",
    "            \"completion_tokens\": result[\"completion_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        }\n",
    "    else:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": f\"ERROR: {result.get('error', 'Unknown error')}\",\n",
    "            \"latency\": None,\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef30d56",
   "metadata": {},
   "source": [
    "### 3. Visualize Results\n",
    "\n",
    "You may see something like this (taken from a previous run) - note how the same prompt has different latency and token usage metrics for different models. In this instance, gpt-4.1 has the lowest total token usage (but the highest latency) - while the o3 reasoning model has the highest token usage (likely due to the reasoning tokens used). Now how the _gpt-4.1-nano_ has the lowest latency (with a slightly higher total token cost) bhile the gpt-4o model is in the middle.\n",
    "\n",
    "While these results are not conclusive, they offer us some intuition into two metrics (token usage and latency) that are key optimization targets for our assistant. **Note** that these results are _not_ grounded in Zava data (and therefore not accurate) - orchestrating a RAG-based solution would incur added token costs (to capture context in prompt) and latency (to retrieve and augment relevant results)\n",
    "\n",
    "| MODEL PERF METRICS | | | |\n",
    "|:---|:---|:---|:---|\n",
    "| Model | Latency (s) | Prompt Tokens |Completion Tokens | Total Tokens\n",
    "gpt-4o\t     | 1.36\t    | 74\t           |  93\t           |    167\n",
    "gpt-4.1\t     | 2.95\t    | 74\t            | 62\t            |    136\n",
    "gpt-4.1-nano | 1.10\t    | 74\t            | 76\t             |   150\n",
    "o3\t         | 1.81\t    | 73\t            | 144\t              |  217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cfd2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ü§ñ MODEL RESPONSES\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left;\">gpt-4o</td>\n",
       "      <td style=\"text-align: left;\">Certainly! A soft, warm gray paint like \"Cloud Drift\" is a fantastic choice for your living room‚Äîit creates a cozy and modern feel while complementing a variety of furniture styles. Look for a satin finish, which is durable and easy to clean. At Zava, similar paint options typically range between...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left;\">gpt-4.1</td>\n",
       "      <td style=\"text-align: left;\">I recommend \"Calm Gray\" for your living room‚Äîa versatile, light gray shade that brightens the space and pairs well with most d√©cor styles. This paint features excellent washability, making it ideal for high-traffic areas. At Zava, our premium interior paint ranges from $28 to $42 per gallon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left;\">gpt-4.1-nano</td>\n",
       "      <td style=\"text-align: left;\">Certainly! I recommend considering \"Soft Sage\" for your living room. It‚Äôs a calming, versatile green hue that creates a welcoming atmosphere. A key feature is its low VOC formula, making it environmentally friendly and safe for indoor use. The price range for a 2.5-gallon can typically falls betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left;\">o3</td>\n",
       "      <td style=\"text-align: left;\">For a versatile, modern look, consider Zava‚Äôs Soft Dove Grey interior paint.‚Ä®\\n‚Ä¢ Key feature: low-VOC, washable finish‚Äîideal for high-traffic areas like living rooms.‚Ä®\\n‚Ä¢ Price range: about $28‚Äì35 per gallon, depending on sheen (matte, eggshell, or satin).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìä MODEL PERFORMANCE METRICS\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Prompt Tokens</th>\n",
       "      <th>Completion Tokens</th>\n",
       "      <th>Total Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>1.79</td>\n",
       "      <td>74</td>\n",
       "      <td>84</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>3.54</td>\n",
       "      <td>74</td>\n",
       "      <td>65</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>0.75</td>\n",
       "      <td>74</td>\n",
       "      <td>83</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o3</td>\n",
       "      <td>2.15</td>\n",
       "      <td>73</td>\n",
       "      <td>150</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ........ Now display the two clean tables\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# First table: Model Responses with left-aligned text\n",
    "response_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    # Truncate long responses for cleaner display\n",
    "    response = data[\"response\"]\n",
    "    if len(response) > 300:\n",
    "        response = response[:297] + \"...\"\n",
    "    response_data.append({\"Model\": model, \"Response\": response})\n",
    "\n",
    "response_df = pd.DataFrame(response_data)\n",
    "print(\"\\n\\nü§ñ MODEL RESPONSES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Custom HTML styling for left-aligned responses\n",
    "html = response_df.to_html(index=False)\n",
    "html = html.replace('<td>ERROR', '<td style=\"color:red\">ERROR')\n",
    "html = html.replace('<td>', '<td style=\"text-align: left;\">')\n",
    "display(HTML(html))\n",
    "\n",
    "# Second table: Performance Metrics\n",
    "metrics_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    metrics_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Latency (s)\": f\"{data['latency']:.2f}\" if data['latency'] is not None else \"N/A\",\n",
    "        \"Prompt Tokens\": data['prompt_tokens'] if data['prompt_tokens'] is not None else \"N/A\",\n",
    "        \"Completion Tokens\": data['completion_tokens'] if data['completion_tokens'] is not None else \"N/A\",\n",
    "        \"Total Tokens\": data['total_tokens'] if data['total_tokens'] is not None else \"N/A\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n\\nüìä MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "display(HTML(metrics_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43046ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's debug why o3-mini is failing\n",
    "# Try invoking the o3-mini model directly to debug\n",
    "# TODO: WHY DOES O3-MINI FAIL TO GENERATE OUTPUT??\n",
    "'''\n",
    "try:\n",
    "    params = {\n",
    "        \"model\": \"o3-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store\"},\n",
    "            {\"role\": \"user\", \"content\": test_prompt}\n",
    "        ],\n",
    "        \"max_completion_tokens\": 300\n",
    "    }\n",
    "    response = client.chat.completions.create(**params)\n",
    "    print(\"o3-mini response:\", response)\n",
    "    print(\"Token usage:\", response.usage)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error invoking o3-mini:\", str(e))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258673f1",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 1. Understand The Requirements\n",
    "\n",
    "Our goal is to make the Cora chatbot **polite, factual, and helpful** to Zava shoppers. But what does this actually mean?\n",
    "\n",
    "1. **Polite & Helpful** - This is about changing the _tone_ and _style_ of responses from Cora to follow a desired template.\n",
    "1. **Factual** - This is about ensuring that responses are _grounded_ in Zava product data, typically using a RAG-based approach.\n",
    "\n",
    "**Desired Tone & Style**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f51ccc",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 2. Explore The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146241e2",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 3. Try Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746e93",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 4. Try Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ff56",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 5. Time To Try Fine-Tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69820d",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; height: 60px; margin: 30px 0; background: linear-gradient(90deg, #ff6ec4 0%, #7873f5 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "    Next: Be More Helpful With SFT\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
