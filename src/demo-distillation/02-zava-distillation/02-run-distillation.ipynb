{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb97771",
   "metadata": {},
   "source": [
    "# Distillation With Fine Tuning - Make It Cost-Effective\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Zava's chatbot is now polite and helpful. But it uses GPT-4.1, which is is too , slow, and expensive for the simple task assigned to it. _Can we get the same behavior, with comparable quality, using a smaller model that would be faster and cheaper?\n",
    "\n",
    "**Process**\n",
    "\n",
    "In this notebook, we'll explore _distillation_, the process by which we use a larger model (GPT-4.1) to teach a smaller model (e.g., GPT-4.1-nano) to do a specific task, with comparable accuracy. It achieves this by trading off the breadth of knowledge of larger models for a _narrower intelligence_ that is more cost-effective. This technique is also referred to as _model compression_ since it effectively reduces the size of model used in the solution. The figure shows the basic workflow we will cover in this lab.\n",
    "\n",
    "_This lab is adapted from the Distilling Sarcasm Lab created by Dave Voutila_.\n",
    "\n",
    "![SFT](./../../../docs/slides/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d18b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Lab Objectives\n",
    "\n",
    "Let's recap our journey so far, in customizing the tone and style of our selected base model (GPT-4.1). With each step, we are improving the quality of responses (tone, style) while optimizing for cost and latency.\n",
    "\n",
    "| Journey | Outcome | \n",
    "|:---|:---|\n",
    "| Prompt Engineering | No changes to GPT-4.1, but limited few-shot examples and longer prompt lengths | \n",
    "| Supervised Fine Tuning | Retrained GPT-4.1 with many Zava Q&A samples - shorter prompts, right tone but slow & expensive| \n",
    "| Distillation | Have GPT-4.1 train smaller, cheaper model - shorter prompts, right tone AND cost-effective | \n",
    "\n",
    "**By the end of this lab you should**\n",
    "- Have a cost-effective model that delivers the required tone & style \n",
    "- Understand how _graders_ work to evaluate quality of model responses\n",
    "- Understand how _distillation_ works to transfer knowledge from \"teacher\" to \"student\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01bcad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Lab Setup\n",
    "\n",
    "**Pre-Requisites**\n",
    "\n",
    "1. You have a deployed Azure AI Foundry project - with a GPT-4.1 model deployed\n",
    "1. You have a .env file - with environment variables updated for the project\n",
    "1. You authenticated with Azure from GitHub Codespaces - using `az login` in the terminal\n",
    "\n",
    "**Process**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6406e9a",
   "metadata": {},
   "source": [
    "### 2.1 Deploy More Models\n",
    "\n",
    "In this lab we'll also learn about graders and how to evaluate different models for the quality of their response, before selecting our teacher and student _candidates_ for distillation. Let's go for a mix of reasoning and non-reasoning models, and for both large and small versions where available.\n",
    "\n",
    "1. Reasoning Models: o3, o3-mini, o4-mini\n",
    "1. Non-Reasoning Models: gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano\n",
    "\n",
    "You will need sufficient quota (TPM) for these models - have a healthy subset of these if you can't deploy them all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ab589",
   "metadata": {},
   "source": [
    "### 2.2 Set Environment Variables\n",
    "\n",
    "Verify that you have set these environment variables, else correct that by updating `.env` and restarting kernel to refresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4488fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required environment variables are set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "required_env_vars = [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_SUBSCRIPTION_ID\",\n",
    "    \"AZURE_RESOURCE_GROUP\",\n",
    "    \"AZURE_AOAI_ACCOUNT\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_env_vars if not os.environ.get(var)]\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "else:\n",
    "    print(\"All required environment variables are set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ee081",
   "metadata": {},
   "source": [
    "### 2.3 Check Libraries\n",
    "\n",
    "Make sure the required Python libraries were installed. This should be done automatically when using GitHub Codespaces from this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74193f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are available!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas\n",
    "    import openai\n",
    "    import dotenv\n",
    "    import azure.identity\n",
    "    import azure.mgmt.cognitiveservices\n",
    "    import IPython\n",
    "    import uuid\n",
    "    import eval_utils\n",
    "    print(\"All required packages are available!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing required package: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ad946",
   "metadata": {},
   "source": [
    "### 2.4 Setup Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d20eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a unique identifier as a run suffix to avoid name collisions over multiple runs\n",
    "import uuid\n",
    "UNIQUE_ID = str(uuid.uuid4()).split(\"-\")[0]\n",
    "print(f\"Run ID: {UNIQUE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b55aff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evaluation-Driven Development\n",
    "\n",
    "We need a *gold standard* sample, not to train our model on, but to measure\n",
    "the efficacy of our grader.\n",
    "\n",
    "We'll use some pre-canned politeness examples from the **Azure OpenAI** tutorial\n",
    "on [fine-tuning a GPT model](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/fine-tune?tabs=command-line).\n",
    "\n",
    "The [baseline.jsonl](./baseline.jsonl) file contains the prompts from that\n",
    "tutorial, but decomposed into a simple format of question/answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ffbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at our baseline *gold standard* dataset.\n",
    "import json\n",
    "\n",
    "rows = 0\n",
    "with open(\"./baseline.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        print(json.loads(line))\n",
    "        rows += 1\n",
    "        if rows >= 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72852df2",
   "metadata": {},
   "source": [
    "## 1. Grading the Grader\n",
    "The Grader is the lynchpin here, so it *must* be effective. \n",
    "\n",
    "Specifically, we'll use a **Score Model Grader** (aka `score_model` via the API).\n",
    "It's job is to take a *prompt* and use it to derive a numeric *score*. We define\n",
    "to decide if the score is \"passing\" or \"failing\", which we'll use as a simple\n",
    "signal of quality.\n",
    "\n",
    "> ‚ÑπÔ∏è **Aside: Garbage In, Garbage Out**\n",
    "> If you cannot explain to a reasoning model like `o3` how to identify high and low\n",
    "> quality responses based on your objective, you have a *shaky foundation* from\n",
    "> which to build upon.\n",
    ">\n",
    "> But keep in mind, `o3` is a *reasoning* model. You do *not* need to give it\n",
    "> fine-grained instructions, but you should give it a *clear objective*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778440a4",
   "metadata": {},
   "source": [
    "### üìÉ Preparing our first Evaluation\n",
    "We'll start with the definition of the prompt for our grader (`GRADER_PROMPT`).\n",
    "\n",
    "Things to note about this prompt:\n",
    "- We let the grader know how we want it to behave, in this case as a politeness\n",
    "  detector and fact checker.\n",
    "- We inform it of what the input data looks like.\n",
    "- We give it a rubrik for scoring each input.\n",
    "- We give it a clear schema for its output to match what a Score Model Grader\n",
    "  must output for the Evaluation system to interpret.\n",
    "\n",
    "And that's it. In this case, we don't need examples, we rely on the reasoning\n",
    "model to decide how to interpret politeness, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the most capable reasoning model as our grader.\n",
    "GRADER_MODEL = \"o3\"\n",
    "\n",
    "# A simple score grader that quantifies politeness while accounting for accuracy\n",
    "# of the model's response. We want the model to answer the question *correctly*\n",
    "# while adjusting their tone. Incorrect answers are heavily penalized.\n",
    "GRADER_PROMPT = \"\"\"\n",
    "You are an expert in politeness detection, while also being a good fact checker.\n",
    "\n",
    "You'll be given a conversation in the form of a question and answer. \n",
    "\n",
    "## Scoring Criteria\n",
    "Judge the answer using two metrics.\n",
    "\n",
    "### Metric 1: How polite is the response?\n",
    "On a scale from 1 to 10 inclusive with 1 being no politeness detected and 10 being\n",
    "the most polite response ever, assign an integer value within that range based\n",
    "on your assessment.\n",
    "\n",
    "### Metric 2: How accurate is the response?\n",
    "Assign a 1 if the response is factually correct. Assign a 0 for this metric if it's\n",
    "incorrect or contains innacuracies.\n",
    "\n",
    "### Final Score\n",
    "The final score you must decide should be based on a weighted blend of Metric 1 and\n",
    "Metric 2 using the formula: `(Metric 1) * (Metric 2)`\n",
    "\n",
    "This means that if Metric 2 is zero, the final score must be zero.\n",
    "\n",
    "## Response Structure\n",
    "Your response must be in a JSON format that can be loaded by Python's json.loads()\n",
    "function. It must resemble the following:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"steps\": [\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    },\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    }\n",
    "  ],\n",
    "  \"result\": <floating point value of the Final Score>\n",
    "}\n",
    "\n",
    "## General Guidance\n",
    "The questions should be simple factual questions with clear answers. Deep research is\n",
    "not required.\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ae533",
   "metadata": {},
   "source": [
    "Now we need to stage our baseline data in the Azure OpenAI service so the grader can\n",
    "access and score each input.\n",
    "\n",
    "Using the SDK, it's a simple file upload, but with a purpose of `evals`.\n",
    "\n",
    "A file used for `evals` ideally is in JSONL format, but does **not** need to be in a\n",
    "chat completions format. We can just use that `{ \"question\": \"?\", \"answer\": \"...\" }`\n",
    "format from our baseline file without any further data engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to first evaluate our grader using a human-curated dataset.\n",
    "# In this case, these are the examples from our fine-tuning tutorial. Let's pretend\n",
    "# we know what the scores for these _should_ be.\n",
    "grader_eval_file = None\n",
    "with open(\"./baseline.jsonl\", \"rb\") as f:\n",
    "    grader_eval_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    grader_eval_file = client.files.wait_for_processing(grader_eval_file.id)\n",
    "\n",
    "print(f\"Created eval file:\\n{grader_eval_file.to_json(indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d3404",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Constructing the Grader\n",
    "For evaluating the grader, we'll be giving it pre-canned prompts for it to score.\n",
    "\n",
    "To do this, we have to provide some templating:\n",
    "\n",
    "1. We give the system a template (`INPUT`) to plug in data from our baseline\n",
    "   jsonl to construct a ficticious prompt from a model.\n",
    "2. We provide a schema to describe the shape of our test data (the baseline file)\n",
    "   in `SCHEMA`.\n",
    "3. Lastly, we define the testing criteria (`TESTING_CRITERIA`) which takes our\n",
    "   prompt template (`INPUT`), the name of our grader model (in Azure OpenAI, this\n",
    "   is the _deployment name_ of the model to use), and the scoring details.\n",
    "\n",
    "This is a lot of data, so it's important to take a moment and wrap your head around\n",
    "this stuff. Remember, this is the _simple_ version of a Score Model Grader! Simple\n",
    "being we're not generating prompts, we're just using a data file to populate a\n",
    "template.\n",
    "\n",
    "So, for example, if we have a row from `baseline.jsonl` that looks like:\n",
    "\n",
    "```json\n",
    "{ \"question\": \"Who spilled coffee on their desk today?\", \"answer\": \"Dave\" }\n",
    "```\n",
    "\n",
    "The _actual_ prompt that will be shown to the _grader_ will be:\n",
    "\n",
    "```json\n",
    "[\n",
    "   { \"role\": \"system\", \"content\": \"You are an expert in politeness detection,...\" },\n",
    "   { \"role\": \"user\", \"content\": \"\\nQ: Who spilled coffee on their desk today?\\nA: Dave\\n\" }\n",
    "]\n",
    "```\n",
    "\n",
    "> ‚ÑπÔ∏è Once you know how to reason about this yourself, you can use something like the\n",
    "> Azure OpenAI Chat Playground to manually test your grader! Just set the system\n",
    "> prompt to your grader prompt and then provide the _user_ content like in the\n",
    "> example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our Evaluation for validating our choice in grader prompt and model.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model in this case, just simple string substitution using this pattern. This\n",
    "# means we directly reference the two fields in our baseline.jsonl file.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines all the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Auto politeness Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],    # Our grader scores in a range from 1 to 10\n",
    "    \"pass_threshold\": 4.0,   # Let's say a 4 is \"passing\" for now.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19368669",
   "metadata": {},
   "source": [
    "### üë®‚Äçüî¨ Putting it together into an Evaluation\n",
    "We have our data source defined (`DATA_SOURCE`) and our testing criteria that defines\n",
    "our grader (`TESTING_CRITERIA`). Now we have what we need to construct an Evaluation.\n",
    "\n",
    "> An evaluation can contain multiple testing criteria (i.e. graders), but in our case\n",
    "> we just use the one above. Just a heads up on why we're giving it a list here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've set up the parameters for our Eval, now we create it via the API.\n",
    "grader_eval = client.evals.create(\n",
    "    name=f\"politeness-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA],\n",
    ")\n",
    "\n",
    "print(f\"‚öñÔ∏è Submitted grader evaluation {grader_eval.id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef0227",
   "metadata": {},
   "source": [
    "Oddly, we're **not done!** \n",
    "\n",
    "We defined the Evaluation, but each Evaluation needs a test Run. The Evaluation defines\n",
    "the general terms, testing criteria, etc., but since we might want to vary some test\n",
    "parameters, we actually need a **Run** to get anything done.\n",
    "\n",
    "Astute readers may have noticed that _we never specified we want to use baseline.jsonl._\n",
    "\n",
    "At the Run-level, we provide the specific test file. This lets you separate out the\n",
    "schema definition of a test file from the data itself, so if you had multiple files\n",
    "you wanted to test, you could create multiple runs.\n",
    "\n",
    "We define `RUN_DATA_SOURCE` below to specify, by _file id_ which file we want this Run\n",
    "to use for data driving our test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our evaluation needs a test run. This is where we let it know to use our\n",
    "# \"gold standard\" file (baseline.jsonl) to test our grader.\n",
    "RUN_DATA_SOURCE = {\n",
    "    \"type\": \"jsonl\",\n",
    "    \"source\": { \"type\": \"file_id\", \"id\": grader_eval_file.id }\n",
    "}\n",
    "grader_run = client.evals.runs.create(\n",
    "    name=f\"politeness-grader-{GRADER_MODEL}\",\n",
    "    eval_id=grader_eval.id,\n",
    "    data_source=RUN_DATA_SOURCE,\n",
    ")\n",
    "print(f\"üèÉ‚Äç‚û°Ô∏è Submitted run {grader_run.id} for {grader_eval.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018b01b",
   "metadata": {},
   "source": [
    "Nothing is instaneous in life, including Evals. Let's wait for our Run to complete.\n",
    "\n",
    "We can do this by polling the status of the Run itself. (Note: we don't poll the Eval.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Eval Run takes time to complete. Let's actively wait for it to finish before continuing.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "while grader_run.status not in [\"completed\", \"failed\"]:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "    now = time.time()\n",
    "    mins, secs = int((now - start_time) // 60), int((now - start_time) % 60)\n",
    "    print(f\"‚è±Ô∏è Elapsed time: {mins} minutes {secs} seconds\")\n",
    "\n",
    "print(f\"üèÅ Run {grader_run.id}: {grader_run.status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c42",
   "metadata": {},
   "source": [
    "### üìä Viewing and Interpreting Results\n",
    "Our Run completed...so now what?\n",
    "\n",
    "You can view the results in Azure AI Foundry, or stay in the comfort of this\n",
    "notebook üòâ. Let's stay in the notebook.\n",
    "\n",
    "There's a help script provided for rendering the results of all Runs for a given\n",
    "list of Evaluations. It will render two things:\n",
    "\n",
    "1. The pass percentage of each Run (i.e. % of prompts scoring above our provided\n",
    "   pass threshold).\n",
    "2. A histogram of individual scores from a Run letting us see how the score\n",
    "   distribution looks to see if we're generating excellent (well-above passing)\n",
    "   results or just barely passing. (_\"C's get degrees,\"_ as they say! üòú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've got a handy script for rendering the results from an Evaluations Runs. Let's\n",
    "# eyeball this stuff. It uses the Evals API to retrieve the scores and plot them.\n",
    "from eval_utils import display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [grader_eval.id], x_range=(0, 10))\n",
    "\n",
    "# We should see that our grader generally thinks our \"gold standard\" is pretty\n",
    "# polite. This is where we'd iterate on tuning the grader, making sure we\n",
    "# clearly capture features for it to score, etc. We're keeping it simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d1a5f",
   "metadata": {},
   "source": [
    "### ü§î So how's it looking?\n",
    "Ok! Not bad if we consider our *gold standard* data as generally \"good\" quality\n",
    "politeness. The Grader thinks these are decent examples with nothing below a `2.0` in\n",
    "this case and with the *p90* score being `6.0` (this varies with each run of the\n",
    "notebook, but it tends to land around here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84f91b",
   "metadata": {},
   "source": [
    "## 2. Baseline Testing of our Base Models\n",
    "At this point we've done two things (hopefully!):\n",
    "\n",
    "1. Developed a Grader we feel is fit for our task.\n",
    "2. Learned how to construct and submit an Eval and Run.\n",
    "\n",
    "> ‚ÑπÔ∏è If you aren't confident in (2), this is a good time to go re-read the above.\n",
    "\n",
    "### Generating our Dataset\n",
    "The real beauty of this type of distillation is we don't have to kill ourselves\n",
    "to create a dataset! We really only need to generate the end-user's side of the\n",
    "prompt and not the model's side, making this _so much simpler._\n",
    "\n",
    "Since our agent usecase here is Coraüõí, we just need some simple questions a\n",
    "user of our application might ask. A model like `GPT-4.1` can do this for us with\n",
    "a prompt like:\n",
    "\n",
    "```\n",
    "Generate 100 question and answer pairs that might be used in a quiz game. Output\n",
    "the pairs in JSONL with the following schema:\n",
    "\n",
    "{ \"item\": { \"question\": <the question>, \"answer\": <the answer> } }\n",
    "\n",
    "Try not to create duplicates!\n",
    "```\n",
    "\n",
    "> Seriously, this is how I created `qa.jsonl`. You can use Github Copilot right\n",
    "> inside Visual Studio Code to generate it and also detect/remove duplicates\n",
    "> until you have 500 rows.\n",
    "\n",
    "Let's take a peek at the data in [qa.jsonl](./qa.jsonl):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfec9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've previously generated 500 Question/Answer pairs. Note that they are only\n",
    "# focused on factual answers. We're not trying to provide any politeness here, just\n",
    "# facts...and honestly we're not going to use the answers anyways...but you might\n",
    "# use them if you had another grader that checked for gold standard answers.\n",
    "import json\n",
    "\n",
    "qa = []\n",
    "with open(\"./qa.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        qa.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of Q/A pairs: {len(qa)}\")\n",
    "for i in range(3):\n",
    "    print(qa[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277e523",
   "metadata": {},
   "source": [
    "We now need to split this into two sets:\n",
    "\n",
    "1. A set for **baseline** testing (`qa_baseline`)\n",
    "2. A set for **validation** after fine-tuning (`qa_validation`)\n",
    "\n",
    "A simple 50/50 split here is good because the point of the validation set here is to have\n",
    "a totally _different_ test to use post-training that has _zero overlap_ with what we'll\n",
    "be using to train our student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's split these into two sets: our baseline set and our validation set. We'll just split\n",
    "# them in half for now. \n",
    "\n",
    "# First we'll randomize it to maybe prove a point that this isn't totally staged üòú\n",
    "from random import shuffle\n",
    "shuffle(qa)\n",
    "\n",
    "# Now we split 50/50.\n",
    "split_at = int(len(qa) / 2)\n",
    "qa_baseline = qa[:split_at]\n",
    "qa_validation = qa[split_at:]\n",
    "\n",
    "# Check it.\n",
    "print(f\"{len(qa_baseline)} pairs for baseline testing, {len(qa_validation)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11cc4b1",
   "metadata": {},
   "source": [
    "Let's upload our baseline dataset.\n",
    "\n",
    "First, we'll write it to disk both to let you inspect it, but also because the OpenAI SDK\n",
    "really prefers to upload data file files. üôÉ\n",
    "\n",
    "> Yeah, if you're a Python dev and know how to make Files-like objects from data in-memory,\n",
    "> _sorry_, but the SDK will barf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our baseline dataset and prepare our Evaluation. We need to save the data\n",
    "# to disk first for...reasons...because of the OpenAI SDK. That's fine.\n",
    "filename = f\"./politeness-baseline-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_baseline:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "baseline_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    baseline_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    baseline_file = client.files.wait_for_processing(baseline_file.id)\n",
    "\n",
    "print(f\"Created baseline file:\\n{baseline_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d2b91",
   "metadata": {},
   "source": [
    "### Defining our Baseline Evaluation\n",
    "Most of this will look similar to above from section 1 where we evaluated our Grader.\n",
    "\n",
    "There are some key differences:\n",
    "\n",
    "1. The prompt template now uses `{{sample.output_text}}` because we'll be using the\n",
    "   base model (the model under test) to generate the answer to the user's question.\n",
    "2. Because we're mixing _both_ the provided dataset file with generated responses,\n",
    "   we have to make some slight tweaks to our data source definition (`DATA_SOURCE`).\n",
    "\n",
    "Other than that, this is pretty similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69774305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now build out the Evaluation details. In this case, we'll *generate* responses\n",
    "# using a base model, unlike before where we used the pre-canned results just to test\n",
    "# the grader.\n",
    "\n",
    "# We'll use a simple system prompt to show how distillation and fine-tuning let us\n",
    "# get away without overly complex prompt engineering.\n",
    "SYSTEM_PROMPT = \"Cora is a Shopping Assistant who is polite and helpful. She always tries to suggest other things that might help the user.\"\n",
    "\n",
    "# We'll use a flee of base models as our baseline, including `o3` (our grader).\n",
    "BASE_MODELS = [\n",
    "    \"o3\",\n",
    "    \"o4-mini\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "]\n",
    "\n",
    "# The prompt we'll grade will look like this pattern. Similar to before, but now we're\n",
    "# going to use {{sample.output_text}} to substitute what the model under test generates.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{sample.output_text}}\n",
    "\"\"\"\n",
    "\n",
    "# Input to our grader remains the same as before, but we reproduce it here for context.\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# The schema and data source are similar, but with one major difference noted below.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    },\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "\n",
    "# Same testing criteria, reproduced again for context.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Auto politeness Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}\n",
    "\n",
    "# We create one Evaluation for *all* our base models. Each model is tested in a\n",
    "# distinct Run that we'll define next.\n",
    "baseline_eval = client.evals.create(\n",
    "    name=f\"sacarsm-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA]\n",
    ")\n",
    "print(f\"‚öñÔ∏è Created baseline eval {baseline_eval.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7640fb",
   "metadata": {},
   "source": [
    "Now instead of a single Run, we submit _one Run per base model_.\n",
    "\n",
    "This part is **new**, so let's look at it closely.\n",
    "\n",
    "Each Run has its own data source defined. Like with the Grader evaluation where\n",
    "we finally said _which_ file to use for a test dataset, we're not providing both\n",
    "a prompt template _and_ a reference to the test dataset.\n",
    "\n",
    "1. `source` -- looks like what we did previously, referencing our test data by\n",
    "   file id.\n",
    "2. `input_messages` -- provides our prompt template, looking similar to how we\n",
    "   defined the Grader prompt previously. Note, however, we're now using our\n",
    "   `SYSTEM_PROMPT` (the simple 1-liner Cora one) and wiring in the test data\n",
    "   as the user's input.\n",
    "\n",
    "> ‚ÑπÔ∏è An astute reader will notice _we're not using the `answer` field_ from our\n",
    "> test data. Yup! In this demo, we don't. You _could_ extend the grader to use\n",
    "> that as \"ground truth\" for scoring the accuracy. Exercise left to you, my\n",
    "> friend!\n",
    "\n",
    "> ‚ö†Ô∏è Attention!\n",
    "> There's one thing to call out and that's the `sampling_params`. This lets us\n",
    "> provide tuning of the chat completion parameters to the model under test. In\n",
    "> this case, we're tuning the max completion tokens.\n",
    "> \n",
    "> However, there are two things to note:\n",
    "> 1. We use a different value depending on if it's a reasoning model being tested\n",
    ">    or if it's a GPT model. (I'm not sure if this matters?)\n",
    "> 2. More importantly, Azure OpenAI has a üêõbug where we need to specificy it as\n",
    ">    `max_completions_tokens` and not `max_completion_tokens`. Note the lack of\n",
    ">    the `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85308636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each run gets its own data source definition as it needs to specify a different\n",
    "# model deployment to use for generation. The template is the prompt template\n",
    "# sent to the model under test. It uses the simple Cora system prompt and for\n",
    "# the user input, we use the \"question\" from the baseline Q&A data file.\n",
    "baseline_runs = []\n",
    "for model in BASE_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": baseline_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 20_000 } if model.startswith(\"o\") else { \"max_completions_tokens\": 100 }, # XXX\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=baseline_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for eval {baseline_eval.id}\")\n",
    "    baseline_runs.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa0f77",
   "metadata": {},
   "source": [
    "Waiting is always the hardest part. What can I say?\n",
    "\n",
    "This can take 15-20 minutes depending on your TPM limits. Maybe more. Maybe less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to wait for our half-dozen or so Runs to finish. Twiddle your thumbs a bit!\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in baseline_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(baseline_runs)):\n",
    "        baseline_runs[i] = client.evals.runs.retrieve(eval_id=baseline_eval.id, run_id=baseline_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {baseline_runs[i].name}: {baseline_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(baseline_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c425e6f",
   "metadata": {},
   "source": [
    "### Interpreting our Baseline Results\n",
    "Now we get to see our winner and loser!\n",
    "\n",
    "We'll use the same plotting function as before to look at _pass percentage_ and the\n",
    "individual score distributions for each model.\n",
    "\n",
    "What we want to do here is:\n",
    "1. Identify the clear winner to designat our *teacher*.\n",
    "2. Identify the clear loser to designate our *student*.\n",
    "\n",
    "Recall our general hypothesis here is:\n",
    "\n",
    "> A larger, more robust model will excel at our task out of the box at the cost of\n",
    "> typically more $'s per token and often higher latency (time-to-first-token).\n",
    ">\n",
    "> A smaller, less robust model will perform poorly out of the box, but will provide\n",
    "> a better $/token price-point and often much lower latency.\n",
    ">\n",
    "> The ideal model for our agent will have the lowest $/token and latency, while\n",
    "> achieving acceptable scores.\n",
    "\n",
    "Our goal, as a reminder, is to take that fast, cheap model and make it perform\n",
    "**as well as the slower, more expensive model**.\n",
    "\n",
    "> üòú Better, Faster, Cheaper: Pick 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our evaluation and identify the best and worst performers.\n",
    "display_evaluation_summary(client, [baseline_eval.id], x_range=(1, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e5d56",
   "metadata": {},
   "source": [
    "You'll notice we have a very clear winner and a very clear loser.\n",
    "\n",
    "- `o3` -- consistently scores the highest both in terms of passing scores, but also\n",
    "  generates generates a lot of `[6, 7, 8]` scores.\n",
    "- `4.1-nano` -- consistently abysmal performance, with the most failing scores in\n",
    "  the range of `[1, 2]`. Just an embarassment!\n",
    "\n",
    "But let's hold that thought for a moment. Visually we can see this, but remember what\n",
    "we want to do here is find not just the _best performing model_, but we need it's best\n",
    "_example responses_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729010c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. üß™ Distilling from the Teacher\n",
    "Let's look at all our models again, this time via code, and find just the _excellent_\n",
    "responses that scored `6.0` or higher.\n",
    "\n",
    "> ‚ÑπÔ∏è This part gets a bit technical! We'll be doing some data engineering on the fly\n",
    "> as we analyze the excellent responses. This is maybe the more complex Python in this\n",
    "> entire notebook as it works around some limitations with the Evaluations API, but\n",
    "> buckle up and I promise it's worth it.\n",
    "\n",
    "What we're going to do is query each Run, look at the Run's _individual results_,\n",
    "collect just the \"excellent\" ones, and while we're doing so we'll be\n",
    "_transforming them into chat completions_.\n",
    "\n",
    "Then it's as simple as seeing which model had the most \"excellent\" scores and\n",
    "declaring our winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll find the model that generated the most \"excellent\" (>= 6.0) examples of politeness.\n",
    "CUTOFF = 6.0\n",
    "HIGH_SCORES = {\n",
    "    \"o3\": [],\n",
    "    \"o4-mini\": [],\n",
    "    \"gpt-4.1\": [],\n",
    "    \"gpt-4.1-mini\": [],\n",
    "    \"gpt-4.1-nano\": [],\n",
    "    \"gpt-4o\": [],\n",
    "    \"gpt-4o-mini\": [],\n",
    "}\n",
    "\n",
    "# Let's find our responses that were Excellent (at or above CUTOFF). We'll collect them\n",
    "# and pre-format them into chat completions format to save time later.\n",
    "#\n",
    "# This part is honestly a bit tricky...we're extracting the prompts and responses for the\n",
    "# model under test and *not* the prompts to the grader, so we have to do surgery. üî™\n",
    "for run in baseline_runs:\n",
    "    pages = client.evals.runs.output_items.list(run.id, eval_id=baseline_eval.id).iter_pages()\n",
    "    for page in pages:\n",
    "        for item in page.data:\n",
    "            # We only used 1 grader. If you use multiple, you should look for which ones you want.\n",
    "            if not item.results:\n",
    "                continue\n",
    "            result = item.results[0]\n",
    "            if result[\"score\"] >= CUTOFF:\n",
    "                generated = result[\"sample\"][\"input\"][-1][\"content\"].strip().split(\"\\nA: \")\n",
    "                question = generated[0][3:] # drops the \"Q: \"\n",
    "                answer = generated[-1]\n",
    "                messages = [\n",
    "                    { \"role\": \"system\", \"content\": SYSTEM_PROMPT },\n",
    "                    { \"role\": \"user\", \"content\": question },\n",
    "                    { \"role\": \"assistant\", \"content\": answer },\n",
    "                ]\n",
    "                HIGH_SCORES[run.model].append({ \"messages\": messages })\n",
    "\n",
    "# Time to find the winner! Obviously, this is probably o3...\n",
    "winning_model = \"\"\n",
    "winning_cnt = 0\n",
    "for key in HIGH_SCORES.keys():\n",
    "    if len(HIGH_SCORES[key]) > winning_cnt:\n",
    "        winning_model = key\n",
    "        winning_cnt = len(HIGH_SCORES[key])\n",
    "        \n",
    "print(f\"üòú Ok! Let's use {winning_model}. It had {winning_cnt} excellent responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bbdd48",
   "metadata": {},
   "source": [
    "We now take _just the Teacher's excellent responses_ and construct our training data.\n",
    "\n",
    "We should have enough for Supervised Fine Tuning if all goes well (based on previous testing of this notebook),\n",
    "so let's give it a train/test split:\n",
    "\n",
    "1. Split the data in-memory.\n",
    "2. Write the training and validation data out as JSONL to disk.\n",
    "3. Upload them via the Files API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70963f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we go any further, let's turn our collected excellent responses into our\n",
    "# training and validation fine-tuning datasets. Like before, we have to write these\n",
    "# to disk and then upload them via the Files API.\n",
    "training_filename = f\"politeness-training-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "validation_filename = f\"politeness-validation-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "# Make an 80/20 split to form our training/validation data.\n",
    "split_at = int(len(HIGH_SCORES[winning_model]) * 0.80)\n",
    "training_data = HIGH_SCORES[winning_model][:split_at]\n",
    "validation_data = HIGH_SCORES[winning_model][split_at:]\n",
    "print(f\"Split into {len(training_data)} training / {len(validation_data)} validation rows.\")\n",
    "\n",
    "# Create and upload the training data.\n",
    "with open(training_filename, \"w\") as f:\n",
    "    for message in training_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(training_filename, \"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    training_file = client.files.wait_for_processing(training_file.id)\n",
    "print(f\"üèãÔ∏è‚Äç‚ôÇÔ∏è Created training file:\\n{training_file.to_json(indent=2)}\")\n",
    "\n",
    "# Create and upload the validation data.\n",
    "with open(validation_filename, \"w\") as f:\n",
    "    for message in validation_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(validation_filename, \"rb\") as f:\n",
    "    validation_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    validation_file = client.files.wait_for_processing(validation_file.id)\n",
    "print(f\"üìã Created validation file:\\n{validation_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce255aed",
   "metadata": {},
   "source": [
    "## 5. üèãÔ∏è‚Äç‚ôÇÔ∏è Training the Student\n",
    "We've got our training dataset, so let's get to fine-tuning!\n",
    "\n",
    "We've chosen `4.1-nano` as the student because it's desperately in the need of some help as\n",
    "shown by its benchmarking.\n",
    "\n",
    "We'll create a job _suffx_ that identifies our teacher model for posterity and include that\n",
    "unique-enough key from before so if you want to run this notebook again you can have two\n",
    "different fine-tuning jobs to compare.\n",
    "\n",
    "> ‚ÑπÔ∏è Look at the use of `extra_body` as a parameter! We're telling Azure OpenAI to try\n",
    "> using Global Training for our job. If you want to learn more about Global Training, check\n",
    "> out the orginal [announcement](https://aka.ms/Build25/FTGlobalAndDev) from **Build 2025**.\n",
    "\n",
    "Note that we're not tuning hyper-parameters here for our job and just using defaults. That\n",
    "is an exercise left for you, dear reader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start training! Submit our fine-tuning job to teach 4.1-nano new tricks.\n",
    "TEACHER_MODEL = winning_model\n",
    "STUDENT_MODEL = \"gpt-4.1-nano-2025-04-14\"\n",
    "SUFFIX = f\"{TEACHER_MODEL}-politeness-{UNIQUE_ENOUGH_KEY}\".replace(\".\", \"\") # '.' is a reserved character üòú\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    model=STUDENT_MODEL,\n",
    "    suffix=SUFFIX,\n",
    "    training_file=training_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    extra_body={ \"trainingType\": \"globalstandard\" }\n",
    ")\n",
    "print(f\"üë®‚Äçüî¨ Created fine-tuning job:\\n{job.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e5e6a",
   "metadata": {},
   "source": [
    "Fine-tuning, like Evaluations, take time. Now's a good time to go catch up on emails ‚úâÔ∏è,\n",
    "walk your dog üêï, or take a nap üò¥. We can poll the training status to know when we can\n",
    "proceed further in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our FT job to complete. You may want to go work on some other tasks for now üòú\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "status = job.status\n",
    "while status not in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "    time.sleep(10)\n",
    "    job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    status = job.status\n",
    "    clear_output(wait=True)\n",
    "    print(f\"üë®‚Äçüî¨ Job {job.id}: {status}\")\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "if status == \"succeeded\":\n",
    "    print(f\"üèÅ Fine-tuning finished!\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Fine-tuning job did not complete successfully (status={status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc761d",
   "metadata": {},
   "source": [
    "Once the job completes, we should eyeball the metrics. Here's what an example\n",
    "should look like:\n",
    "\n",
    "`TODO: INSERT IMAGE`\n",
    "\n",
    "If our `4.1-nano` model is learning, you should see the _training loss_ decreasing.\n",
    "This gives us a sense of if the model learned something, but it **doesn't** tell us\n",
    "if the model is fit for production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0660a",
   "metadata": {},
   "source": [
    "## 6. üßë‚Äç‚öñÔ∏è Judging our Student against its Peers\n",
    "Shipping this new model to production right now would be truly a YOLO moment and I\n",
    "cannot recommend it.\n",
    "\n",
    "What we really need to do now is go back and benchmark it against the original\n",
    "`4.1-nano` base model.\n",
    "\n",
    "Remember how we split that initial Q&A dataset into two parts? The `qa_validation`\n",
    "file contains unseen questions for our models that could not possible appear in the\n",
    "training data, so it's the perfect check to answer the question if we actually\n",
    "moved the needle.\n",
    "\n",
    "### üö¢ Deploying our New Model\n",
    "Azure OpenAI requires you to \"deploy\" a model in order to have an endpoint to call\n",
    "it, so let's do that.\n",
    "\n",
    "We need to reach for a different SDK, though, but no worries! The Azure Cognitive\n",
    "Services SDK let's us talk to the Azure OpenAI control plane to deploy things\n",
    "without having to leave this comfy notebook.\n",
    "\n",
    "> ‚ö†Ô∏è The code below uses a `DefaultAzureCredential`. The easiest way to make sure\n",
    "> one exists is to have installed and authenticated the Azure CLI tooling.\n",
    ">\n",
    "> See: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest\n",
    "\n",
    "We'll use the new Azure OpenAI `Developer Tier` available for Fine-Tuned models as\n",
    "it's purpose built for model candidate evaluation: you only pay per token at the\n",
    "same base model rates! (Learn more [here](https://aka.ms/Build25/FTGlobalAndDev).)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE THIS LATER - THIS IS A HACK TO GET ENV VAR INTO RUNNING NOTEBOOK\n",
    "os.environ[\"AZURE_SUBSCRIPTION_ID\"] = \"\"\n",
    "os.environ[\"AZURE_RESOURCE_GROUP\"] = \"nitya-aitour26-ftbrk-rg\"\n",
    "os.environ[\"AZURE_AOAI_ACCOUNT\"] = \"nitya-aitour26-ftbrk-resource\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to deploy our fine-tuned model. We'll use Developer Tier to keep\n",
    "# costs under control for evaluation.\n",
    "\n",
    "# We can't do this with the OpenAI SDK, so we need to reach for the Azure SDK.\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "\n",
    "cogsvc_client = CognitiveServicesManagementClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    ")\n",
    "\n",
    "# Define our Deployment. Note the use of SKU for specificy capacity and\n",
    "# the name of the deployment tier.\n",
    "DEPLOYMENT_NAME = f\"politeness-distilled-{SUFFIX}\"\n",
    "DEPLOYMENT = {\n",
    "    \"properties\": {\n",
    "        \"model\": { \n",
    "            \"format\": \"OpenAI\", \n",
    "            \"name\": job.fine_tuned_model, \n",
    "            \"version\": \"1\" \n",
    "        },\n",
    "    },\n",
    "    \"sku\": { \n",
    "        \"capacity\": 250, \n",
    "        \"name\": \"DeveloperTier\" \n",
    "    },\n",
    "}\n",
    "\n",
    "# Submit the request for provisioning. This may take a few minutes, so we\n",
    "# poll for updates. If it already exists, this should return quickly. Since\n",
    "# we're deploying a 4.1-nano model, this should only take 3-5 minutes tops.\n",
    "deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "    resource_group_name=os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    account_name=os.environ.get(\"AZURE_AOAI_ACCOUNT\"),\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    deployment=DEPLOYMENT,\n",
    ")\n",
    "print(f\"üõ≥Ô∏è Submitted deployment {deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9affa8",
   "metadata": {},
   "source": [
    "Deployments, like Evaluations and Training, are not instaneous, but they are typically\n",
    "a lot faster! `4.1-nano` typically will deploy in 3-4 minutes, so maybe don't walk\n",
    "away from your desk just yet üòú."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our deployment to finish provisioning.\n",
    "start_time = time.time()\n",
    "status = deployment.status()\n",
    "\n",
    "while status not in [\"Succeeded\", \"Failed\"]:\n",
    "    deployment.wait(5)\n",
    "    status = deployment.status()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"üõ≥Ô∏è Provisioning {DEPLOYMENT_NAME}: {status}\")\n",
    "    print(\"‚è±Ô∏èElapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ Provisioning finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdda88",
   "metadata": {},
   "source": [
    "### ‚¨ÜÔ∏è Uploading the Validation Data\n",
    "This should be old-hat by now! We'll take the in-memory validatation data (`qa_validation`)\n",
    "and create a dataset via the Files API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our post-training validation dataset and prepare our final Evaluation.\n",
    "# We need to save the data to disk first, again for...reasons.\n",
    "filename = f\"./politeness-posttraining-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_validation:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "posttraining_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    posttraining_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    posttraining_file = client.files.wait_for_processing(posttraining_file.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35180760",
   "metadata": {},
   "source": [
    "### üèÉ Creating Runs to make our Student Compete with the Base Model\n",
    "Here's where Evaluations really shine ‚ú®!\n",
    "\n",
    "We _could_ reuse our baseline evaluation from before and just add new Runs so all the\n",
    "results are in one place and easily viewable.\n",
    "\n",
    "In this case, we'll keep it simple and create a new Evaluation primarily because the\n",
    "visualization code already has color-coding based on Eval, not Run and I'm too lazy\n",
    "to rewrite it further üòú.\n",
    "\n",
    "Most of this is repeat, in fact we could have reused thigns like `DATA_SOURCE`,\n",
    "`TESTING_CRITERIA`, etc. but restate them below for completeness. Same can be said\n",
    "for the Runs.\n",
    "\n",
    "*However*, we'll focus on just comparing our new model to `4.1-nano` and also add in\n",
    "`4.1` as sort of a control. We're testing with totally new data here, unseen by any\n",
    "of our models, so what we expect to see is:\n",
    "\n",
    "1. The base `4.1-nano` performs poorly again.\n",
    "2. The base `4.1` performs decently and on-par with the previous baseline.\n",
    "3. The new student model outperforms ideally both, but at least `4.1-nano`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a final Eval using our post-training dataset that doesn't overlap with the\n",
    "# original distillation and training dataset. This lets us judge our new model based on\n",
    "# data it hasn't seen before. We'll also through in one of our better performing base\n",
    "# models as a control.\n",
    "POST_EVAL_MODELS = [\n",
    "    DEPLOYMENT_NAME,\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"gpt-4.1\",\n",
    "]\n",
    "\n",
    "# SCHEMA, GRADER_MODEL, and INPUT are re-used from our previous Evaluation definition,\n",
    "# but let's restate the source and testing criteria again.\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Auto politeness Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}\n",
    "posttraining_eval = client.evals.create(\n",
    "    name=f\"politeness-posttrain-evaluation-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA]\n",
    ")\n",
    "print(f\"Created eval {posttraining_eval.id}\")\n",
    "\n",
    "# Now add our runs.\n",
    "postraining_runs = []\n",
    "for model in POST_EVAL_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": posttraining_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 100 }, # XXX again, note the purposeful typo\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=posttraining_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for {posttraining_eval.id}\")\n",
    "    postraining_runs.append(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de42c84",
   "metadata": {},
   "source": [
    "Again, we wait! ‚è±Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67769d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we wait for our runs to finish.\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in postraining_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(postraining_runs)):\n",
    "        postraining_runs[i] = client.evals.runs.retrieve(eval_id=posttraining_eval.id, run_id=postraining_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {postraining_runs[i].name}: {postraining_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(postraining_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482042a",
   "metadata": {},
   "source": [
    "### üìä Interpreting the Post-Training Results\n",
    "Let's first look at _just the new Evaluation_ to see if our new model outperforms its\n",
    "competition (`4.1-nano` and `4.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our post-training evaluation. Fingers crossed!\n",
    "display_evaluation_summary(client, [posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfe89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's put it all together.\n",
    "# Let's visualize our post-training evaluation. Fingers crossed!\n",
    "display_evaluation_summary(client, [baseline_eval.id, posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af014a",
   "metadata": {},
   "source": [
    "## ü•≥ 7. Celebrate or Cry\n",
    "It _should_ be time to celebrate if all went well!\n",
    "\n",
    "We can clearly see our new model `politeness-distill` is outperforming all but `o3` in terms\n",
    "of pass-percentage, but maybe most importantly has adopted the high-scoring ability of\n",
    "`o3`, generating scores at `6.0` or above!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99b11c",
   "metadata": {},
   "source": [
    "## üîö 8. Conclusion\n",
    "In this notebook we demonstrated **distillation** using Azure OpenAI *Evaluations*\n",
    "and *Fine Tuning* features.\n",
    "\n",
    "We used an objective of *adjusting the tone* of a model to meet our needs, in this\n",
    "case making its responses polite, while preserving accuracy in results, and\n",
    "*distilled* the native capabilities of a state-of-the-art reasoning model (`o3`)\n",
    "into a much smaller, non-reasoning model (`4.1-nano`) to let our agent or app\n",
    "use the smallest model possible while:\n",
    "\n",
    "- ü§ë minimizing per-token costs\n",
    "- üèéÔ∏è improve performance (latency)\n",
    "\n",
    "We did all this:\n",
    "\n",
    "- without creating training data directly\n",
    "- without knowing the ideal student model\n",
    "- only by knowing how to define our Grader\n",
    "\n",
    "So to wrap it all up:\n",
    "\n",
    "1. We described the ideal state to our complex reasoning model in the form of\n",
    "   a few samples we feel are ideal.\n",
    "2. We described to the reasoning model (grader) how to judge those examples\n",
    "   to measure their quality.\n",
    "3. We let Evaluations and Fine Tuning do the rest!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fdd24",
   "metadata": {},
   "source": [
    "## üßπ 9. Cleanup\n",
    "If you want to clean up everthing from this notebook, set `cleanup = True` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad63a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to clean up your mess? \n",
    "cleanup = False\n",
    "\n",
    "if cleanup:\n",
    "    pass # TODO: add in calls to nuke everything"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
