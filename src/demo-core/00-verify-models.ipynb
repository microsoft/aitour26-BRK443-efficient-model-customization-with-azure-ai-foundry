{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b9b1da",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">Step 1: Plan For Zava Scenario</h1>\n",
    "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "    Zava is an enterprise retailer that sells home improvement goods to DIY enthusiasts.\n",
    "    Cora is their AI customer support chatbot that helps customers find relevant products.\n",
    "    The Zava AI Engineering team wants to make sure Cora is: helpful, precise, and cost-effective to operate.\n",
    "    In this set of demos we'll learn how they approach the model customization journey, to make this happen.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a1a23",
   "metadata": {},
   "source": [
    "## 1. Deploy Models\n",
    "\n",
    "This is \"Act 2\" of the breakout session where we look at core Fine-Tuning options in Azure AI Foundry. To achieve this:\n",
    "\n",
    "1. We need a base model to fine-tune - for SFT\n",
    "2. We need a teacher model and a student model to transfer behavioral knowledge - for Distillation.\n",
    "\n",
    "To achieve this, we have provisioned a few model \"candidates\" by default, using the setup process defined earlier. \n",
    "\n",
    "You should see some subset of these models in that list.\n",
    "\n",
    "- Reasoning Models ‚Üí o3, o3-mini, o4-mini\n",
    "- Chat Models ‚Üí  gpt-4o, gpt-4.1, gpt-4.1-nano\n",
    "- Embedding Models ‚Üí  text-embedding-3-large\n",
    "\n",
    "Your `.env` variables should already be set to reflect the desired Azure AI Foundry project environment. Let's go!\n",
    "\n",
    "\n",
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f640e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5e58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Setup an Azure OpenAI client and test out different models\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-05-01-preview\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396a202",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 1. Define Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25becf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Define a test prompt that we'll use for all models\n",
    "test_prompt = \"\"\"\n",
    "You are a home improvement assistant for Zava, a fictional hardware store.\n",
    "Please give me a brief recommendation for a paint color for my living room.\n",
    "Include one key feature of the paint and a price range.\n",
    "\"\"\"\n",
    "\n",
    "# List of models to test\n",
    "models_to_test = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"o3\",\n",
    "    \"o3-mini\",\n",
    "    \"o4-mini\"\n",
    "]\n",
    "\n",
    "# Function to call a model and measure performance\n",
    "def test_model(model_name, prompt):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add the appropriate token limit parameter based on model type\n",
    "        if model_name.startswith(\"o\"):\n",
    "            params[\"max_completion_tokens\"] = 300\n",
    "        else:\n",
    "            params[\"max_tokens\"] = 300\n",
    "        \n",
    "        response = client.chat.completions.create(**params)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        \n",
    "        # Extract response and token usage\n",
    "        content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"latency\": latency,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"response\": content\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with model {model_name}: {str(e)}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f61e2",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 2. Run Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372c618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting model tests...\n",
      "..... Testing model: gpt-4o\n",
      "..... Testing model: gpt-4o\n",
      "..... Testing model: gpt-4o-mini\n",
      "..... Testing model: gpt-4o-mini\n",
      "..... Testing model: gpt-4.1-mini\n",
      "..... Testing model: gpt-4.1-mini\n",
      "..... Testing model: gpt-4.1-nano\n",
      "..... Testing model: gpt-4.1-nano\n",
      "..... Testing model: o3\n",
      "..... Testing model: o3\n",
      "..... Testing model: o3-mini\n",
      "..... Testing model: o3-mini\n",
      "..... Testing model: o4-mini\n",
      "..... Testing model: o4-mini\n",
      "\n",
      "‚úÖ Completed testing 7 models\n",
      "\n",
      "‚úÖ Completed testing 7 models\n"
     ]
    }
   ],
   "source": [
    "# Test each model with the same prompt\n",
    "# Reset results to avoid duplicates when re-running this cell\n",
    "import sys\n",
    "\n",
    "print(\"üîÑ Starting model tests...\", flush=True)\n",
    "\n",
    "results = []\n",
    "detailed_outputs = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        print(f\"..... Testing model: {model}\")\n",
    "        result = test_model(model, test_prompt)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception outside test_model for {model}: {str(e)}\", file=sys.stderr)\n",
    "        results.append({\"model\": model, \"error\": str(e)})\n",
    "\n",
    "# Store the detailed output for later, but don't display all of it \n",
    "for result in results:\n",
    "    if \"error\" not in result:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": result[\"response\"],\n",
    "            \"latency\": result[\"latency\"],\n",
    "            \"prompt_tokens\": result[\"prompt_tokens\"],\n",
    "            \"completion_tokens\": result[\"completion_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        }\n",
    "    else:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": f\"ERROR: {result.get('error', 'Unknown error')}\",\n",
    "            \"latency\": None,\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úÖ Completed testing {len(detailed_outputs)} models\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef30d56",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 3. Visualize Results\n",
    "\n",
    "You may see something like this (taken from a previous run) - note how the same prompt has different latency and token usage metrics for different models. In this instance, gpt-4.1 has the lowest total token usage (but the highest latency) - while the o3 reasoning model has the highest token usage (likely due to the reasoning tokens used). Now how the _gpt-4.1-nano_ has the lowest latency (with a slightly higher total token cost) bhile the gpt-4o model is in the middle.\n",
    "\n",
    "While these results are not conclusive, they offer us some intuition into two metrics (token usage and latency) that are key optimization targets for our assistant. **Note** that these results are _not_ grounded in Zava data (and therefore not accurate) - orchestrating a RAG-based solution would incur added token costs (to capture context in prompt) and latency (to retrieve and augment relevant results)\n",
    "\n",
    "| MODEL PERF METRICS | | | |\n",
    "|:---|:---|:---|:---|\n",
    "| Model | Latency (s) | Prompt Tokens |Completion Tokens | Total Tokens\n",
    "gpt-4o\t     | 1.36\t    | 74\t           |  93\t           |    167\n",
    "gpt-4.1\t     | 2.95\t    | 74\t            | 62\t            |    136\n",
    "gpt-4.1-nano | 1.10\t    | 74\t            | 76\t             |   150\n",
    "o3\t         | 1.81\t    | 73\t            | 144\t              |  217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296ad974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-4.1-mini': {'completion_tokens': 74,\n",
      "                  'latency': 1.0304079055786133,\n",
      "                  'prompt_tokens': 74,\n",
      "                  'response': 'For your living room, I recommend a soft, warm '\n",
      "                              'gray paint like \"Cozy Pebble.\" A key feature of '\n",
      "                              'this paint is its excellent light-reflecting '\n",
      "                              'quality, which brightens the room while '\n",
      "                              'maintaining a cozy atmosphere. The price '\n",
      "                              'typically ranges from $25 to $40 per gallon. '\n",
      "                              'This makes it a great balance of quality and '\n",
      "                              'affordability for home use.',\n",
      "                  'total_tokens': 148},\n",
      " 'gpt-4.1-nano': {'completion_tokens': 69,\n",
      "                  'latency': 0.8667013645172119,\n",
      "                  'prompt_tokens': 74,\n",
      "                  'response': 'Certainly! For your living room, I recommend a '\n",
      "                              'soft, neutral beige paint. A key feature of '\n",
      "                              'this color is its versatility, allowing it to '\n",
      "                              'complement various furniture styles and color '\n",
      "                              'schemes. The price range for quality beige '\n",
      "                              'paint typically falls between $25 and $45 per '\n",
      "                              'gallon. Would you like suggestions on specific '\n",
      "                              'paint brands or additional color options?',\n",
      "                  'total_tokens': 143},\n",
      " 'gpt-4o': {'completion_tokens': 70,\n",
      "            'latency': 0.7760438919067383,\n",
      "            'prompt_tokens': 74,\n",
      "            'response': 'Certainly! For a living room, I recommend using *Zava '\n",
      "                        'Classic Calm Beige*, a warm and neutral color that '\n",
      "                        'creates a welcoming and versatile atmosphere. This '\n",
      "                        'paint features excellent coverage, minimizing the '\n",
      "                        'number of coats needed. Prices range from **$25 to '\n",
      "                        '$40 per gallon**, depending on the finish (matte, '\n",
      "                        'eggshell, or satin).',\n",
      "            'total_tokens': 144},\n",
      " 'gpt-4o-mini': {'completion_tokens': 81,\n",
      "                 'latency': 1.451134204864502,\n",
      "                 'prompt_tokens': 74,\n",
      "                 'response': 'I recommend using a soft, warm gray such as '\n",
      "                             '\"Repose Gray\" by Sherwin-Williams for your '\n",
      "                             'living room. This color creates a cozy and '\n",
      "                             'inviting atmosphere while complementing a '\n",
      "                             'variety of decor styles. A key feature of this '\n",
      "                             'paint is its excellent durability and '\n",
      "                             'washability, making it ideal for high-traffic '\n",
      "                             'areas. The price range for a gallon is typically '\n",
      "                             'between $30 to $50.',\n",
      "                 'total_tokens': 155},\n",
      " 'o3': {'completion_tokens': 93,\n",
      "        'latency': 1.967181921005249,\n",
      "        'prompt_tokens': 73,\n",
      "        'response': 'For a warm, inviting living room, consider Zava‚Äôs ‚ÄúCalm '\n",
      "                    'Clay‚Äù interior latex paint‚Äîa soft, neutral beige-taupe '\n",
      "                    'that pairs easily with most d√©cor styles. Key feature: '\n",
      "                    'it‚Äôs a low-VOC, washable finish, so everyday scuffs wipe '\n",
      "                    'right off. Price range: about $32‚Äì$38 per gallon, '\n",
      "                    'depending on sheen.',\n",
      "        'total_tokens': 166},\n",
      " 'o3-mini': {'completion_tokens': 300,\n",
      "             'latency': 2.279433012008667,\n",
      "             'prompt_tokens': 73,\n",
      "             'response': '',\n",
      "             'total_tokens': 373},\n",
      " 'o4-mini': {'completion_tokens': 300,\n",
      "             'latency': 2.078922986984253,\n",
      "             'prompt_tokens': 73,\n",
      "             'response': '',\n",
      "             'total_tokens': 373}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(detailed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfd2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ü§ñ MODEL RESPONSES\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"text-align: left;\">Model</th>\n",
       "      <th style=\"text-align: left;\">Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">gpt-4o</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">Certainly! For a living room, I recommend using *Zava Classic Calm Beige*, a warm and neutral color that creates a welcoming and versatile atmosphere. This paint features excellent coverage, minimizing the number of coats needed. Prices range from **$25 to $40 per gallon**, depending on the finis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">gpt-4o-mini</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">I recommend using a soft, warm gray such as \"Repose Gray\" by Sherwin-Williams for your living room. This color creates a cozy and inviting atmosphere while complementing a variety of decor styles. A key feature of this paint is its excellent durability and washability, making it ideal for high-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">gpt-4.1-mini</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">For your living room, I recommend a soft, warm gray paint like \"Cozy Pebble.\" A key feature of this paint is its excellent light-reflecting quality, which brightens the room while maintaining a cozy atmosphere. The price typically ranges from $25 to $40 per gallon. This makes it a great balance o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">gpt-4.1-nano</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">Certainly! For your living room, I recommend a soft, neutral beige paint. A key feature of this color is its versatility, allowing it to complement various furniture styles and color schemes. The price range for quality beige paint typically falls between $25 and $45 per gallon. Would you like su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">o3</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">For a warm, inviting living room, consider Zava‚Äôs ‚ÄúCalm Clay‚Äù interior latex paint‚Äîa soft, neutral beige-taupe that pairs easily with most d√©cor styles. Key feature: it‚Äôs a low-VOC, washable finish, so everyday scuffs wipe right off. Price range: about $32‚Äì$38 per gallon, depending on sheen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">o3-mini</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\">o4-mini</td>\n",
       "      <td style=\"text-align: left; vertical-align: top; padding: 8px;\"></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìä MODEL PERFORMANCE METRICS\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Prompt Tokens</th>\n",
       "      <th>Completion Tokens</th>\n",
       "      <th>Total Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.78</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1.45</td>\n",
       "      <td>74</td>\n",
       "      <td>81</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>1.03</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>0.87</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o3</td>\n",
       "      <td>1.97</td>\n",
       "      <td>73</td>\n",
       "      <td>93</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o3-mini</td>\n",
       "      <td>2.28</td>\n",
       "      <td>73</td>\n",
       "      <td>300</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o4-mini</td>\n",
       "      <td>2.08</td>\n",
       "      <td>73</td>\n",
       "      <td>300</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ........ Now display the two clean tables\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "# First table: Model Responses with left-aligned text\n",
    "response_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    # Truncate long responses for cleaner display\n",
    "    response = data[\"response\"]\n",
    "    if len(response) > 300:\n",
    "        response = response[:297] + \"...\"\n",
    "    response_data.append({\"Model\": model, \"Response\": response})\n",
    "\n",
    "response_df = pd.DataFrame(response_data)\n",
    "print(\"\\n\\nü§ñ MODEL RESPONSES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Display with proper escaping - using pandas built-in HTML rendering\n",
    "html_output = response_df.to_html(index=False, escape=True)\n",
    "html_output = html_output.replace('<td>ERROR', '<td style=\"color:red\">ERROR')\n",
    "html_output = html_output.replace('<table', '<table style=\"width:100%\"')\n",
    "html_output = html_output.replace('<th>', '<th style=\"text-align: left;\">')\n",
    "# Apply left alignment to all data cells\n",
    "html_output = html_output.replace('<td>', '<td style=\"text-align: left; vertical-align: top; padding: 8px;\">')\n",
    "display(HTML(html_output))\n",
    "\n",
    "# Second table: Performance Metrics\n",
    "metrics_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    metrics_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Latency (s)\": f\"{data['latency']:.2f}\" if data['latency'] is not None else \"N/A\",\n",
    "        \"Prompt Tokens\": data['prompt_tokens'] if data['prompt_tokens'] is not None else \"N/A\",\n",
    "        \"Completion Tokens\": data['completion_tokens'] if data['completion_tokens'] is not None else \"N/A\",\n",
    "        \"Total Tokens\": data['total_tokens'] if data['total_tokens'] is not None else \"N/A\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n\\nüìä MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "display(HTML(metrics_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d14a5",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 100%); color: white; padding: 15px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;\">\n",
    "    <h2 style=\"color: #FFF; text-shadow: 1px 1px 2px rgba(0,0,0,0.4); margin: 0;\"> 2Ô∏è‚É£ | Understand The Requirements </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258673f1",
   "metadata": {},
   "source": [
    "Our goal is to make the Cora chatbot **polite, factual, and helpful** to Zava shoppers. But what does this actually mean?\n",
    "\n",
    "1. **Polite & Helpful** - This is about changing the _tone_ and _style_ of responses from Cora to follow a desired template.\n",
    "1. **Factual** - This is about ensuring that responses are _grounded_ in Zava product data, typically using a RAG-based approach.\n",
    "\n",
    "**Desired Tone & Style**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f51ccc",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 3Ô∏è‚É£ | Explore The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146241e2",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 4Ô∏è‚É£ | Try Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746e93",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 5Ô∏è‚É£ | Try Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ff56",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 6Ô∏è‚É£ | Time To Try Fine-Tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69820d",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "   Next: Be More Helpful With SFT\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
